{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.integrate import odeint\n",
    "import networkx as nx\n",
    "\n",
    "import scipy.stats as sp_s\n",
    "\n",
    "import pybel as pb\n",
    "\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create generic discrete probability function\n",
    "class cg_node():\n",
    "    def __init__(self,n_inputs,name):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.name = name\n",
    "        \n",
    "        if n_inputs == 0:\n",
    "            self.label = 'exogenous'\n",
    "        else:\n",
    "            self.label = 'endogenous'\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def p_init(self,input_data,var_data):\n",
    "        \n",
    "        self.n_data = len(input_data)\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.var_data = var_data\n",
    "        \n",
    "        if self.n_inputs == 0:\n",
    "            p_ave = np.zeros(3)\n",
    "            n_count = self.n_data\n",
    "            for i in range(0,3):\n",
    "                p_ave[i] = np.sum(var_data == i-1)/n_count\n",
    "        \n",
    "        elif self.n_inputs == 1:\n",
    "            n_count = np.zeros(3)\n",
    "            p_ave = np.zeros((3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                n_count[i] = np.sum(input_data == i-1)\n",
    "                for j in range(0,3):\n",
    "                    p_ave[j,i] = np.sum((input_data[:,0] == i-1)*(var_data == j-1))/n_count[i]\n",
    "            \n",
    "            \n",
    "        elif self.n_inputs == 2:\n",
    "            n_count = np.zeros((3,3))\n",
    "            p_ave = np.zeros((3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    n_count[i,j] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1))\n",
    "                    for k in range(0,3):\n",
    "                        p_ave[k,i,j] = np.sum(\n",
    "                            (input_data[:,0] == i-1)*(input_data[:,1] == j-1)*(var_data == k-1))/n_count[i,j]\n",
    "                        \n",
    "        elif self.n_inputs == 3:\n",
    "            n_count = np.zeros((3,3,3))\n",
    "            p_ave = np.zeros((3,3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    for k in range(0,3):\n",
    "                        n_count[i,j,k] = np.sum(\n",
    "                            (input_data[:,0] == i-1)*(input_data[:,1] == j-1)*(input_data[:,2] == k-1))\n",
    "                        for m in range(0,3):\n",
    "                            p_ave[m,i,j,k] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                *(input_data[:,2] == k-1)*(var_data == m-1))/n_count[i,j,k]\n",
    "                            \n",
    "        elif self.n_inputs == 4:\n",
    "            n_count = np.zeros((3,3,3,3))\n",
    "            p_ave = np.zeros((3,3,3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    for k in range(0,3):\n",
    "                        for m in range(0,3):\n",
    "                            n_count[i,j,k,m] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                *(input_data[:,2] == k-1)*(input_data[:,3] == m-1))\n",
    "                            for q in range(0,3):\n",
    "                                p_ave[q,i,j,k,m] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                    *(input_data[:,2] == k-1)*(input_data[:,3] == m-1)\n",
    "                                    *(var_data == q-1))/n_count[i,j,k,m]\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            print('error -- too many inputs')\n",
    "            return\n",
    "            \n",
    "        self.n_count = torch.tensor(n_count/self.n_data)\n",
    "        self.prob_dist = torch.tensor(p_ave)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sample(self,data_in=[]):\n",
    "        \n",
    "        if self.n_inputs == 0:\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = self.prob_dist)).bool()\n",
    "        elif self.n_inputs == 1:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0]])\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = p_temp)).bool()\n",
    "        elif self.n_inputs == 2:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0],data_in[1]])\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = p_temp)).bool()\n",
    "        elif self.n_inputs == 3:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0],data_in[1],data_in[2]])\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = p_temp)).bool()\n",
    "        else:\n",
    "            print('error -- too many inputs')\n",
    "            samp_out = []\n",
    "        \n",
    "        return samp_out\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cg_graph():\n",
    "    \n",
    "    def __init__(self,str_list=[],bel_graph=[]):\n",
    "        \n",
    "        edge_list = []\n",
    "\n",
    "        entity_list = []\n",
    "        \n",
    "        if str_list:\n",
    "\n",
    "            for item in str_list:\n",
    "\n",
    "                sub_ind = item.find('=')\n",
    "\n",
    "                sub_temp = item[:sub_ind-1]\n",
    "                obj_temp = item[sub_ind+3:]\n",
    "                \n",
    "                rel_temp = item[sub_ind:sub_ind+2]\n",
    "\n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "\n",
    "                edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "        elif bel_graph:\n",
    "            \n",
    "            for item in bel_graph.edges:\n",
    "                edge_temp = bel_graph.get_edge_data(item[0],item[1],item[2])\n",
    "                sub_temp = str(item[0])\n",
    "                obj_temp = str(item[1])\n",
    "                rel_temp = edge_temp['relation']\n",
    "                \n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "\n",
    "        n_nodes = len(entity_list)\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        adj_mat = np.zeros((n_nodes,n_nodes),dtype=int)\n",
    "\n",
    "        for item in edge_list:\n",
    "            out_ind = entity_list.index(item[0])\n",
    "            in_ind = entity_list.index(item[1])\n",
    "            adj_mat[out_ind,in_ind] = 1\n",
    "            \n",
    "        self.edge_list = edge_list\n",
    "        self.entity_list = entity_list\n",
    "        self.adj_mat = adj_mat\n",
    "        \n",
    "        self.graph = nx.DiGraph(adj_mat)\n",
    "        \n",
    "        node_dict = {}\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            node_dict[entity_list[i]] = cg_node(np.sum(adj_mat[:,i]),entity_list[i])\n",
    "        \n",
    "        self.node_dict = node_dict\n",
    "        \n",
    "        self.cond_list = []\n",
    "        \n",
    "        self.sample_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = []\n",
    "        self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(self.adj_mat[:,i] > 0)[0] for i in range(0,n_nodes)]\n",
    "        self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,n_nodes)]\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            self.parent_name_dict[entity_list[i]] = [entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            self.child_name_dict[entity_list[i]] = [entity_list[item] for item in self.child_ind_list[i]]\n",
    "\n",
    "        # create rank-3 delta tensor\n",
    "        tensor_temp = torch.zeros((3,3,3)).double()\n",
    "        for i in range(0,3):\n",
    "            tensor_temp[i,i,i] = 1\n",
    "            \n",
    "        self.tensor_temp = tensor_temp\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def prob_init(self,data_in):\n",
    "        # initialize all of the nodes\n",
    "        \n",
    "        exog_list = []\n",
    "        prob_dict = {}\n",
    "        \n",
    "        for name in self.node_dict:\n",
    "            i = self.entity_list.index(name)\n",
    "            data_in_temp = data_in[:,self.parent_ind_list[i]]\n",
    "            data_out_temp = data_in[:,i]\n",
    "            \n",
    "            self.node_dict[name].p_init(data_in_temp,data_out_temp)\n",
    "            \n",
    "            if self.node_dict[name].n_inputs == 0:\n",
    "                exog_list.append(name)\n",
    "            prob_dict[name] = self.node_dict[name].prob_dist\n",
    "        \n",
    "        self.exog_list = exog_list\n",
    "        self.prob_dict = prob_dict\n",
    "\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def sample_vars(self,names,flag=0):\n",
    "        # do a multi-variable sample\n",
    "        \n",
    "        # sample only those variables w/o sample data\n",
    "        if np.any([item in self.sample_dict for item in names]):\n",
    "            sample_list = [item for item in names if item not in self.sample_dict]\n",
    "            if sample_list:\n",
    "                self.sample_vars(sample_list,flag+1)\n",
    "        \n",
    "        # sample exogenous variables\n",
    "        elif np.any([item in self.exog_list for item in names]):\n",
    "            in_exog = [item for item in names if item in self.exog_list]\n",
    "            not_in_exog = [item for item in names if item not in self.exog_list]\n",
    "            \n",
    "            for item in in_exog:\n",
    "                self.sample_dict[item] = self.node_dict[item].sample()\n",
    "            if not_in_exog:              \n",
    "                self.sample_vars(not_in_exog,flag+1)\n",
    "            \n",
    "        # if you have samples from all of the parents, sample names\n",
    "        # otherwise, sample the parents\n",
    "        elif names:\n",
    "            \n",
    "            sample_list = []\n",
    "            sample_list2 = []\n",
    "            \n",
    "            for item in names:\n",
    "                parent_list = self.parent_name_dict[item]\n",
    "                \n",
    "                if np.all([item2 in self.sample_dict for item2 in parent_list]):\n",
    "                    self.sample_dict[item] = self.node_dict[item].sample(\n",
    "                        [self.sample_dict[item2] for item2 in parent_list])\n",
    "                else:\n",
    "                    sample_list = sample_list + [item2 for item2 in parent_list if item2 not in sample_list]\n",
    "                    sample_list2 = sample_list2 + [item]\n",
    "            if sample_list:\n",
    "                self.sample_vars(sample_list,flag+1)\n",
    "            if sample_list2:\n",
    "                self.sample_vars(sample_list2,flag+1)\n",
    "            \n",
    "        # if you're back at the root node, return the samples\n",
    "        # otherwise, don't return anything - the values are stored in self.sample_dict\n",
    "        if flag == 0:\n",
    "            \n",
    "            tensor_sample = torch.Tensor([-1,0,1]).int()\n",
    "            \n",
    "            output =[tensor_sample[self.sample_dict[item]] for item in names]\n",
    "            self.sample_dict = {}\n",
    "            return output\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "\n",
    "    def gen_path_nodes(self,sources,destinations):\n",
    "        \n",
    "        source_inds = [self.entity_list.index(item) for item in sources]\n",
    "        dest_inds = [self.entity_list.index(item) for item in destinations]\n",
    "        \n",
    "        nodes = []\n",
    "        \n",
    "        for i in source_inds:\n",
    "            for j in dest_inds:      \n",
    "                for path in nx.all_simple_paths(self.graph, source=i, target=j):\n",
    "                    for ind in path:\n",
    "                        if self.entity_list[ind] not in nodes:\n",
    "                            nodes.append(self.entity_list[ind])\n",
    "                            \n",
    "        return nodes\n",
    "        \n",
    "    \n",
    "    def joint_dist_add(self,add_node,nodes_temp,prob_temp):\n",
    "        # create a new joint distribution with add_node now included\n",
    "        \n",
    "        # deliberately let out 'a' - for the variable being added\n",
    "        str_temp = 'bcdefghijklmnopqrstuvwxyz'\n",
    "        \n",
    "        # find parent indices\n",
    "        par_inds = [nodes_temp.index(item2) for item2 in self.parent_name_dict[add_node]]\n",
    "\n",
    "        n_inds = len(par_inds)\n",
    "\n",
    "        if n_inds == 1:\n",
    "            str1 = 'ay,' + str_temp[par_inds[0]] + 'yz,'\n",
    "            str2 = str_temp[:len(nodes_temp)].replace(str_temp[par_inds[0]],'z')\n",
    "\n",
    "            str_sum = str1 + str2\n",
    "            prob_out = torch.einsum(str_sum,self.prob_dict[add_node],self.tensor_temp,prob_temp)\n",
    "\n",
    "        elif n_inds == 2:                        \n",
    "            str1 = 'awy,' + str_temp[par_inds[0]] + 'wx,' + str_temp[par_inds[1]] + 'yz,'\n",
    "\n",
    "            str2 = str_temp[:len(nodes_temp)].replace(\n",
    "                str_temp[par_inds[0]],'x').replace(str_temp[par_inds[1]],'z')\n",
    "\n",
    "            str_sum = str1 + str2\n",
    "            prob_out = torch.einsum(str_sum,self.prob_dict[add_node],self.tensor_temp,self.tensor_temp,prob_temp)\n",
    "\n",
    "        elif n_inds == 3:\n",
    "            str1 = ('auwy,' + str_temp[par_inds[0]] + 'uv,' + str_temp[par_inds[1]] \n",
    "                    + 'wx,' + str_temp[par_inds[2]] + 'yz,')\n",
    "\n",
    "            str2 = str_temp[:len(nodes_temp)].replace(\n",
    "                str_temp[par_inds[0]],'u').replace(str_temp[par_inds[1]],'x').replace(\n",
    "                str_temp[par_inds[2]],'z')\n",
    "\n",
    "            str_sum = str1 + str2\n",
    "            prob_out = torch.einsum(str_sum,\n",
    "                self.prob_dict[add_node],self.tensor_temp,self.tensor_temp,self.tensor_temp,prob_temp)\n",
    "            \n",
    "        else:\n",
    "            print('too many parents')\n",
    "            prob_out = prob_temp\n",
    "            \n",
    "        return prob_out\n",
    "        \n",
    "    \n",
    "    def calc_prob(self,names):\n",
    "        # calculate the joint probability over a list of named nodes\n",
    "        \n",
    "        # find all paths from exogenous nodes        \n",
    "        path_nodes = self.gen_path_nodes(self.exog_list,names)\n",
    "        \n",
    "        for item in names:\n",
    "            if item in self.exog_list and item not in path_nodes:\n",
    "                path_nodes.append(item)\n",
    "        \n",
    "        print(path_nodes)\n",
    "\n",
    "        # get joint exogenous probability distribution\n",
    "        nodes_temp = []\n",
    "        for item in self.exog_list:\n",
    "            if item in path_nodes:\n",
    "                nodes_temp.append(item)\n",
    "                \n",
    "        #print(nodes_temp)\n",
    "        #print(self.exog_list)\n",
    "        prob_temp = self.prob_dict[nodes_temp[0]]\n",
    "        for item in nodes_temp[1:]:\n",
    "            prob_temp = torch.einsum('...i,j',prob_temp,self.prob_dict[item])\n",
    "        \n",
    "        # identify all of the children of nodes_temp in path_nodes\n",
    "        child_nodes = []\n",
    "        for item in nodes_temp:\n",
    "            for item2 in self.child_name_dict[item]:\n",
    "                if item2 in path_nodes and item2 not in child_nodes:\n",
    "                    child_nodes.append(item2)\n",
    "            \n",
    "        flag = 0\n",
    "        \n",
    "        # iterate through node children until target nodes are reached\n",
    "        while flag == 0:\n",
    "            #print(nodes_temp)\n",
    "            \n",
    "            # determine which nodes to add\n",
    "            # all children of nodes_temp in path_nodes, not in nodes_temp, and that have all their parents in\n",
    "            # nodes_temp\n",
    "            add_nodes = []\n",
    "            for item in nodes_temp:\n",
    "                for item2 in self.child_name_dict[item]:\n",
    "                    if (item2 in path_nodes\n",
    "                        and item2 not in nodes_temp\n",
    "                        and np.all([item3 in nodes_temp for item3 in self.parent_name_dict[item2]])\n",
    "                        and item2 not in add_nodes):\n",
    "                        add_nodes.append(item2)\n",
    "            \n",
    "            #print(add_nodes)\n",
    "\n",
    "            # add nodes to the joint distribution\n",
    "            for item in add_nodes:\n",
    "                prob_temp = self.joint_dist_add(item,nodes_temp,prob_temp)\n",
    "\n",
    "                # add the new node to nodes_temp\n",
    "                nodes_temp = [item] + nodes_temp\n",
    "                \n",
    "            # determine which nodes to subtract\n",
    "            # all nodes in nodes_temp not in names and that have all their children in nodes_temp\n",
    "            sub_nodes = []\n",
    "            for item in nodes_temp:\n",
    "                \n",
    "                child_path_list = []\n",
    "                \n",
    "                for item2 in self.child_name_dict[item]:\n",
    "                    if item2 in path_nodes:\n",
    "                        child_path_list.append(item2)\n",
    "\n",
    "                if item not in names and np.all([item2 in nodes_temp for item2 in child_path_list]):\n",
    "                    sub_nodes.append(item)\n",
    "                    \n",
    "            #print(sub_nodes)\n",
    "                    \n",
    "            # sum over the sub_nodes probabilities\n",
    "            \n",
    "            if sub_nodes:\n",
    "                remove_indices = [nodes_temp.index(item) for item in sub_nodes]\n",
    "                prob_temp = torch.sum(prob_temp,dim=remove_indices)\n",
    "            \n",
    "                # remove summed nodes from nodes_temp\n",
    "                for item in sub_nodes:\n",
    "                    nodes_temp.remove(item)\n",
    "                    \n",
    "            if sorted(nodes_temp) == sorted(names):\n",
    "                flag = 1\n",
    "                permute_inds = [nodes_temp.index(item) for item in names] \n",
    "                prob_temp = prob_temp.permute(permute_inds)\n",
    "\n",
    "            #print()\n",
    "            \n",
    "            \n",
    "        return prob_temp\n",
    "    \n",
    "    \n",
    "    def calc_cond_prob(self,cond_prob,uncond_prob):\n",
    "        \n",
    "        # check to make sure the lists don't overlap\n",
    "        if np.any([item in uncond_prob for item in cond_prob]):\n",
    "            print('error -- overlapping lists')\n",
    "            return\n",
    "        \n",
    "        n_cond = len(cond_prob)\n",
    "        n_uncond = len(cond_prob)\n",
    "        \n",
    "        p_joint = self.calc_prob(cond_prob + uncond_prob)\n",
    "        p_uncond = self.calc_prob(uncond_prob)\n",
    "                \n",
    "        if n_uncond == 1:\n",
    "            p_cond = torch.einsum('...i,ijk,j',p_joint,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 2:\n",
    "            p_cond = torch.einsum('...il,ijk,lmn,jm',p_joint,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 3:\n",
    "            p_cond = torch.einsum('...ilp,ijk,lmn,pqr,jmq',\n",
    "                p_joint,self.tensor_temp,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        else:\n",
    "            print('too many conditioned variables')\n",
    "            p_cond = p_joint\n",
    "            \n",
    "        return p_cond\n",
    "            \n",
    "    \n",
    "    def calc_do(self,names,do_vars,do_vals):\n",
    "        # calculate the final probability distribution of the variable in question given do variables\n",
    "        \n",
    "        # add do_vars to list of exogenous variables\n",
    "        self.exog_list += do_vars\n",
    "        \n",
    "        # sever links from do_vars to parents\n",
    "        child_temp = self.child_name_dict\n",
    "        \n",
    "        for item in self.child_name_dict:\n",
    "            for item2 in do_vars:\n",
    "                if item2 in self.child_name_dict[item]:\n",
    "                    self.child_name_dict[item].remove(item2)\n",
    "        \n",
    "        names_prob_dict = {}\n",
    "        for item in do_vars:\n",
    "            names_prob_dict[item] = self.prob_dict[item]\n",
    "        \n",
    "        # specify distributions for those do_vars\n",
    "        for item in do_vars:\n",
    "            p_temp = np.zeros(3)\n",
    "            p_temp[do_vals[item]+1] = 1\n",
    "            self.prob_dict[item] = torch.Tensor(p_temp)\n",
    "            \n",
    "        \n",
    "        prob_out = self.calc_prob(names)\n",
    "        \n",
    "        # restore original list of exogenous variables\n",
    "        for item in do_vars:\n",
    "            self.exog_list.remove(item)\n",
    "        \n",
    "        # restore original probability distributions\n",
    "        for item in do_vars:\n",
    "            self.prob_dict[item] = names_prob_dict[item]\n",
    "        \n",
    "        # restore original child dictionary\n",
    "        self.child_name_dict = child_temp        \n",
    "        \n",
    "        return prob_out\n",
    "    \n",
    "        \n",
    "    def calc_do_cond(self,do_vars,do_vals,cond_prob,uncond_prob):\n",
    "        \n",
    "        # check to make sure the lists don't overlap\n",
    "        if (np.any([item in uncond_prob for item in cond_prob]) \n",
    "            or np.any([item in do_vars for item in cond_prob])\n",
    "            or np.any([item in do_vars for item in uncond_prob])):\n",
    "            print('error -- overlapping lists')\n",
    "            return\n",
    "        \n",
    "        n_cond = len(cond_prob)\n",
    "        n_uncond = len(cond_prob)\n",
    "            \n",
    "        \n",
    "        p_joint = self.calc_do(cond_prob + uncond_prob,do_vars,do_vals)\n",
    "        p_uncond = self.calc_do(uncond_prob,do_vars,do_vals)\n",
    "                \n",
    "        if n_uncond == 1:\n",
    "            p_cond = torch.einsum('...i,ijk,j',p_joint,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 2:\n",
    "            p_cond = torch.einsum('...il,ijk,lmn,jm',p_joint,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 3:\n",
    "            p_cond = torch.einsum('...ilp,ijk,lmn,pqr,jmq',\n",
    "                p_joint,self.tensor_temp,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        else:\n",
    "            print('too many conditioned variables')\n",
    "            p_cond = p_joint\n",
    "        \n",
    "        return p_cond\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    def calc_counterfact():\n",
    "        return\n",
    "    \n",
    "    def calc_cde(self,names,do_vars,do_vals,ctrl_vars,ctrl_vals):\n",
    "        tot_vars = do_vars + ctrl_vars\n",
    "        tot_vals = {}\n",
    "        tot_ctrl_vals = {}\n",
    "        for item in do_vars:\n",
    "            tot_vals[item] = do_vals[item]\n",
    "            tot_ctrl_vals = ctrl_vals[item]\n",
    "            \n",
    "        for item in ctrl_vars:\n",
    "            tot_vals[item] = 0\n",
    "            tot_ctrl_vals[item] = ctrl_vals[item]\n",
    "            \n",
    "        return self.calc_do(names,tot_vars,tot_vals) - self.calc_do(names,tot_vars,tot_ctrl_vals)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def calc_te(self,names,do_vars,do_vals):\n",
    "        \n",
    "        do_vals_0 = {}\n",
    "        for item in do_vals:\n",
    "            do_vals_0[item] = 0\n",
    "            \n",
    "        return self.calc_do(names,do_vars,do_vals) - self.calc_do(do_vars,do_vals_0)\n",
    "    \n",
    "    def calc_nde(self,names,do_vars,do_vals):\n",
    "        \n",
    "        # identify parents of names\n",
    "        parent_list = []\n",
    "        for item in names:\n",
    "            for item2 in self.parent_name_dict[item]:\n",
    "                if item2 not in parent_list:\n",
    "                    parent_list.append(item2)\n",
    "                    \n",
    "        par_do_vals = []\n",
    "        for item in do_vars:\n",
    "            if item in parent_list:\n",
    "                par_do_vals.append(item)\n",
    "        \n",
    "        if not par_do_vals:\n",
    "            print('no direct effect')\n",
    "            prob_out = 0\n",
    "        \n",
    "        else:\n",
    "            # calculate probability of non-do_var parents given do_vars = 0\n",
    "            \n",
    "            non_do_parents = [item for item in parent_list if item not in do_vars]\n",
    "            do_vals_0 = {}\n",
    "            for item in do_vars:\n",
    "                do_vals_0[item] = 0\n",
    "            \n",
    "            non_par_do_vars = []\n",
    "            for item in do_vars:\n",
    "                if item not in parent_list:\n",
    "                    non_par_do_vars.append(item)\n",
    "            \n",
    "            nodes_temp = non_do_parents\n",
    "            \n",
    "            prob_temp = self.calc_do(non_do_parents,do_vars,do_vals_0)\n",
    "            # do outer product to get overall distribution\n",
    "            for item in non_par_do_vars:\n",
    "                p_temp = np.zeros(3)\n",
    "                p_temp[do_vals[item]] = 1\n",
    "                p_add = torch.Tensor(p_temp)\n",
    "                prob_temp = torch.einsum('...i,j',prob_temp,p_add)\n",
    "                nodes_temp.append(item)\n",
    "            \n",
    "            n_sum = len(nodes_temp)\n",
    "            n_names = len(names)\n",
    "            for item in names:\n",
    "                prob_temp = self.joint_dist_add(item,nodes_temp,prob_temp)\n",
    "                \n",
    "            # calculate overall joint probability distribution\n",
    "            sum_axes = range(n_names,n_names+n_sum)\n",
    "            prob_do_vals = np.sum(prob_temp,axis=tuple(sum_axes))\n",
    "            \n",
    "            prob_out = prob_do_vals - self.calc_do(names,do_vars,do_vals_0)\n",
    "\n",
    "        return prob_out\n",
    "    \n",
    "    def calc_nie():\n",
    "        # identify parents of names\n",
    "        parent_list = []\n",
    "        for item in names:\n",
    "            for item2 in self.parent_name_dict[item]:\n",
    "                if item2 not in parent_list:\n",
    "                    parent_list.append(item2)\n",
    "                    \n",
    "        par_do_vals = []\n",
    "        for item in parent_list:\n",
    "            if item in do_vars:\n",
    "                par_do_vals.append(item)\n",
    "        \n",
    "        if np.all([item in do_vars for item in parent_list]):\n",
    "            print('no indirect effect')\n",
    "            prob_out = 0\n",
    "        \n",
    "        else:\n",
    "            # calculate probability of non-do_var parents given do_vars = do_vals\n",
    "            \n",
    "            non_do_parents = [item for item in parent_list if item not in do_vars]\n",
    "            do_vals_0 = {}\n",
    "            for item in do_vars:\n",
    "                do_vals_0[item] = 0\n",
    "            \n",
    "            non_par_do_vars = []\n",
    "            for item in do_vars:\n",
    "                if item not in parent_list:\n",
    "                    non_par_do_vars.append(item)\n",
    "            \n",
    "            nodes_temp = non_do_parents\n",
    "            \n",
    "            prob_temp = self.calc_do(non_do_parents,do_vars,do_vals)\n",
    "            # do outer product to get overall distribution\n",
    "            for item in non_par_do_vars:\n",
    "                p_temp = np.zeros(3)\n",
    "                p_temp[1] = 1\n",
    "                p_add = torch.Tensor(p_temp)\n",
    "                prob_temp = torch.einsum('...i,j',prob_temp,p_add)\n",
    "                nodes_temp.append(item)\n",
    "            \n",
    "            n_sum = len(nodes_temp)\n",
    "            n_names = len(names)\n",
    "            for item in names:\n",
    "                prob_temp = self.joint_dist_add(item,nodes_temp,prob_temp)\n",
    "                \n",
    "            # calculate overall joint probability distribution\n",
    "            sum_axes = range(n_names,n_names+n_sum)\n",
    "            prob_do_vals = np.sum(prob_temp,axis=tuple(sum_axes))\n",
    "            \n",
    "            prob_out = prob_do_vals - self.calc_do(names,do_vars,do_vals_0)\n",
    "\n",
    "        return prob_out\n",
    "\n",
    "    def cond_mut_info(self,target,test,cond,data_in):\n",
    "        \n",
    "        cond_temp = cond\n",
    "        \n",
    "        if not cond:\n",
    "            # find parents of target\n",
    "            for item in target:\n",
    "                for item2 in self.parent_name_dict[item]:\n",
    "                    if item2 not in cond_temp:\n",
    "                        cond_temp.append(item2)\n",
    "        \n",
    "        \n",
    "        target_inds = [self.entity_list.index(item) for item in target]\n",
    "        test_inds = [self.entity_list.index(item) for item in test]\n",
    "        cond_inds = [self.entity_list.index(item) for item in cond_temp]\n",
    "        \n",
    "        n_total = len(data_in)\n",
    "        \n",
    "        null_joint = data_in[:,target_inds + cond_inds]\n",
    "        null_cond = data_in[:,cond_inds]\n",
    "        \n",
    "        hypth_joint = data_in[:,target_inds + test_inds + cond_inds]\n",
    "        hypth_cond = data_in[:,test_inds + cond_inds]\n",
    "        \n",
    "        null_entropy = 0\n",
    "        null_list = []\n",
    "        \n",
    "        hypth_entropy = 0\n",
    "        hypth_list = []\n",
    "        for i in range(0,n_total):\n",
    "\n",
    "            if np.all([np.any(null_joint[i,:] != item) for item in null_list]):\n",
    "                num_sum = np.sum([np.all(null_joint[i,:] == item) for item in null_joint])\n",
    "                denom_sum = np.sum([np.all(null_cond[i,:] == item) for item in null_cond])\n",
    "                null_entropy = null_entropy - num_sum*np.log(num_sum/denom_sum)\n",
    "                null_list.append(null_joint[i,:])\n",
    "            \n",
    "            if np.all([np.any(hypth_joint[i,:] != item) for item in hypth_list]):\n",
    "                num_sum = np.sum([np.all(hypth_joint[i,:] == item) for item in hypth_joint])\n",
    "                denom_sum = np.sum([np.all(hypth_cond[i,:] == item) for item in hypth_cond])\n",
    "                hypth_entropy = hypth_entropy - num_sum*np.log(num_sum/denom_sum)\n",
    "                hypth_list.append(hypth_joint[i,:])\n",
    "                \n",
    "        return (null_entropy - hypth_entropy)/n_total\n",
    "        \n",
    "    def g_test(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest\n",
    "        \n",
    "        p_name = self.calc_prob(name)*len(data_in)\n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        name_data = data_in[:,name_ind]\n",
    "        \n",
    "        p_data = torch.Tensor([np.sum(name_data == -1),np.sum(name_data == 0),np.sum(name_data == 1)])\n",
    "        \n",
    "        print(p_name)\n",
    "        print(p_data)\n",
    "        \n",
    "        g_val = 2*torch.sum(p_data*torch.log(p_data/p_name))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "    \n",
    "    def g_test_emp(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest\n",
    "        \n",
    "        #p_name = self.calc_prob(name)*len(data_in)\n",
    "        # generate an empirical distribution for variable name\n",
    "        model_data = np.zeros(len(data_in))\n",
    "        for i in range(0,len(data_in)):\n",
    "            model_data[i] = self.sample_vars(name)[0].item()\n",
    "            \n",
    "        p_model = torch.Tensor([np.sum(model_data == -1),np.sum(model_data == 0),np.sum(model_data == 1)])\n",
    "        print(p_model)\n",
    "        \n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        name_data = data_in[:,name_ind]\n",
    "        \n",
    "        p_data = torch.Tensor([np.sum(name_data == -1),np.sum(name_data == 0),np.sum(name_data == 1)])\n",
    "        print(p_data)\n",
    "        \n",
    "        g_val = 2*torch.sum(p_data*torch.log(p_data/p_model))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "    \n",
    "    def write_to_cf(self,filename):\n",
    "        # write the causal graph to a text file to import into causal fusion\n",
    "        \n",
    "        pos_dict = nx.drawing.layout.planar_layout(self.graph)\n",
    "        node_filler1 = (',\"label\":\"\",\"shape\":\"ellipse\",\"fontSize\":14,\"sizeLabelMode\":5,\"font\":{\"size\":14}'\n",
    "            + ',\"size\":14,\"labelNodeId\":\"node')\n",
    "        \n",
    "        \n",
    "        node_filler2 = ('ID\",\"labelNodeOffset\":{\"x\":0,\"y\":0},'\n",
    "            + '\"labelOffset\":{\"x\":0,\"y\":0},\"shadow\":{\"color\":\"#00000080\",\"size\":0,\"x\":0,\"y\":0}}},')\n",
    "        \n",
    "        edge_filler = '\",\"type\":\"directed\",\"metadata\":{\"isLabelDraggable\":true,\"label\":\"\"}},'\n",
    "        \n",
    "        txt_file = open(filename + '.txt','w',newline='')\n",
    "        \n",
    "        str_list = []\n",
    "        \n",
    "        str_list.append('{\"name\":\"bel_graph\",')\n",
    "        str_list.append('\"nodes\":[')\n",
    "        \n",
    "        # write nodes\n",
    "        for i in range(0,len(self.entity_list)):\n",
    "            name = self.entity_list[i]\n",
    "            str_temp = ('{\"id\":\"node' + str(i) + '\",\"name\":\"' + name + '\",\"label\":\"' + name \n",
    "                + '\",\"type\":\"basic\",\"metadata\":{\"x\":' + str(pos_dict[i][0]*100) + ',\"y\":' \n",
    "                + str(pos_dict[i][1]*100))\n",
    "            \n",
    "            if i == len(self.entity_list) - 1:\n",
    "                str_list.append(str_temp + node_filler1 + str(i) + node_filler2[:-1])\n",
    "            else:\n",
    "                str_list.append(str_temp + node_filler1 + str(i) + node_filler2)\n",
    "            \n",
    "        \n",
    "        str_list.append('],')\n",
    "        \n",
    "        str_list.append('\"edges\":[')\n",
    "        \n",
    "        # write edges\n",
    "        for i in range(0,len(self.edge_list)):\n",
    "            item = self.edge_list[i]\n",
    "            from_node = self.entity_list.index(item[0])\n",
    "            to_node = self.entity_list.index(item[1])\n",
    "            \n",
    "            str_temp = ('{\"id\":\"node' + str(from_node) + '->node' + str(to_node) + '\",\"from\":\"'\n",
    "                + item[0] + '\",\"to\",\"' + item[1])\n",
    "            \n",
    "            if i == len(self.edge_list) - 1:\n",
    "                str_list.append(str_temp + edge_filler[:-1])\n",
    "            else:\n",
    "                str_list.append(str_temp + edge_filler)\n",
    "                        \n",
    "        \n",
    "        str_list.append('],')\n",
    "        str_list.append('\"task\":{}')\n",
    "        str_list.append('\"metadata\":{},')\n",
    "        \n",
    "        str_list.append('\"project_id\":\"88D621AF72E0634C\",\"_fileType\":\"graph\"}')\n",
    "        \n",
    "        \n",
    "        [txt_file.writelines(item) for item in str_list]\n",
    "        \n",
    "        txt_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bel_temp = pb.from_bel_script('temp.txt')\n",
    "graph_test = cg_graph(bel_graph=bel_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dir(graph_test))\n",
    "print()\n",
    "for item in graph_test.edge_list:\n",
    "    print(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAE1CAYAAACWU/udAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-0.90909091 -0.34848485]\n",
      "1\n",
      "[ 1.         -0.34848485]\n",
      "2\n",
      "[-0.63636364 -0.07575758]\n",
      "3\n",
      "[ 0.72727273 -0.07575758]\n",
      "4\n",
      "[-0.5        -0.21212121]\n",
      "5\n",
      "[0.18181818 0.46969697]\n",
      "6\n",
      "[-0.22727273  0.06060606]\n",
      "7\n",
      "[0.18181818 0.33333333]\n",
      "8\n",
      "[0.18181818 0.1969697 ]\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "nx.draw_planar(graph_test.graph)\n",
    "plt.show()\n",
    "\n",
    "pos_dict = nx.drawing.layout.planar_layout(graph_test.graph)\n",
    "for item in pos_dict:\n",
    "    print(item)\n",
    "    print(pos_dict[item])\n",
    "graph_test.write_to_cf('sag_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str_list = ['temp =| cloudy','cloudy => rainy','temp => icream','rainy =| icream']\n",
    "graph_test = cg_graph(str_list=str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print([graph_test.node_dict[item].name,graph_test.node_dict[item].n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_test.prob_init(tot_data)\n",
    "print(graph_test.exog_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(item)\n",
    "    print(graph_test.node_dict[item].n_count)\n",
    "    print(graph_test.node_dict[item].prob_dist)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test.sample_vars(['icream','rainy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph_test.calc_prob(['rainy','icream'])\n",
    "y = graph_test.calc_prob(['icream'])\n",
    "y2 = graph_test.calc_prob(['rainy'])\n",
    "print(x)\n",
    "print()\n",
    "print(y)\n",
    "print(torch.sum(x,dim=0))\n",
    "\n",
    "print()\n",
    "print(y2)\n",
    "print(torch.sum(x,dim=1))\n",
    "\n",
    "# this is somehow reversed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = graph_test.calc_cond_prob(['rainy'],['icream'])\n",
    "print()\n",
    "print(z)\n",
    "print()\n",
    "print(torch.matmul(z,y))\n",
    "\n",
    "print(torch.sum(z,dim=1))\n",
    "\n",
    "print(torch.sum(z,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_dict = {}\n",
    "do_dict['rainy'] = torch.Tensor([1]).int()\n",
    "for item in do_dict:\n",
    "    print(do_dict[item])\n",
    "    print(graph_test.prob_dict[item])\n",
    "\n",
    "\n",
    "print(graph_test.exog_list)\n",
    "a1 = graph_test.calc_do(['icream'],['rainy'],do_dict)\n",
    "print(a1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_test.calc_do_cond(['rainy'],do_dict,['temp'],['icream'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in graph_test.prob_dict:\n",
    "    print(item)\n",
    "    print(graph_test.prob_dict[item])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_test.cond_mut_info(['rainy'],['temp'],['cloudy'],tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(graph_test.gen_path_nodes(graph_test.exog_list,['temp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_test.prob_dict['icream'])\n",
    "\n",
    "a = graph_test.g_test(['icream'],tot_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(graph_test.prob_dict['cloudy'][:,graph_test.node_dict['temp'].sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_test.prob_dict['cloudy'])\n",
    "print(graph_test.calc_prob(['cloudy']))\n",
    "print(torch.matmul(graph_test.prob_dict['cloudy'],graph_test.prob_dict['temp']))\n",
    "print(torch.matmul(graph_test.prob_dict['temp'],graph_test.prob_dict['cloudy']))\n",
    "print()\n",
    "\n",
    "a = graph_test.g_test_emp(['icream'],tot_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pyro.sample('',pyro.distributions.Multinomial(probs = torch.Tensor([0.3,0.2,0.5]))).bool()\n",
    "#samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = self.prob_dist))\n",
    "print(x)\n",
    "y = torch.Tensor([-1,0,1])[x]\n",
    "print(y)\n",
    "\n",
    "z = torch.Tensor([1]).int()\n",
    "print(z)\n",
    "print(z+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(graph_test.calc_prob(['icream']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(graph_test))\n",
    "print()\n",
    "print(graph_test.entity_list)\n",
    "print(graph_test.adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp.stats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indep_vars(n_samples):\n",
    "    \n",
    "    T_list = []\n",
    "    C_list = []\n",
    "    P_list = []\n",
    "    \n",
    "    for i in range(0,n_samples):\n",
    "        \n",
    "        #x = pyro.sample(\"x_{}\".format(i), pyro.distributions.Normal(20,5))\n",
    "        \n",
    "        #T_temp = pyro.distributions.Normal(20,5).sample()\n",
    "        #C_temp = 0.5*pyro.distributions.Beta(1,1+T_temp/10).sample() + 0.5*pyro.distributions.Uniform(0,1).sample()\n",
    "        #P_temp = (0.5*pyro.distributions.Exponential(1).sample() \n",
    "            #+ 0.5*pyro.distributions.Exponential(1/(C_temp+1)).sample())\n",
    "        \n",
    "        T_list.append(pyro.sample(\"T_{}\".format(i), pyro.distributions.Normal(20,5)))\n",
    "        \n",
    "        C_list.append(0.5*pyro.sample(\"C1_{}\".format(i),pyro.distributions.Beta(1,1+T_list[-1]/10)) \n",
    "            + 0.5*pyro.sample(\"C2_{}\".format(i),pyro.distributions.Uniform(0,1)))\n",
    "        P_list.append(0.5*pyro.sample(\"P1_{}\".format(i), pyro.distributions.Exponential(1))\n",
    "            + 0.5*pyro.sample(\"P2.{}\".format(i),pyro.distributions.Exponential(1/(C_list[-1]+1))))\n",
    "        \n",
    "    return T_list,C_list,P_list\n",
    "\n",
    "def dep_vars(T_list,C_list,P_list):\n",
    "    \n",
    "    n_pts = len(T_list)\n",
    "    \n",
    "    I_list = []\n",
    "    \n",
    "    for i in range(0,n_pts):\n",
    "        \n",
    "        T_temp = T_list[i]\n",
    "        C_temp = C_list[i]\n",
    "        P_temp = P_list[i]\n",
    "        \n",
    "        if P_temp > 2.5 or T_temp < 15:\n",
    "            I_list.append(1e-6*pyro.sample(\"I_{}\".format(i),pyro.distributions.Bernoulli(1)))\n",
    "        else:\n",
    "            I_list.append(pyro.sample(\"I_{}\".format(i),\n",
    "                pyro.distributions.Beta(2*(2.5-P_temp)*(T_temp-12)/(2.5*12),2)))\n",
    "            #I_temp = torch.tensor(0.5)\n",
    "        \n",
    "    return I_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "temp,cloud,precip = indep_vars(n_data)\n",
    "icream = dep_vars(temp,cloud,precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trinarize data relative to baseline\n",
    "T_base = 20\n",
    "C_base = 0.38\n",
    "P_base = 1.2\n",
    "I_base = 0.23\n",
    "\n",
    "T_sig = 1.0\n",
    "C_sig = 0.02\n",
    "P_sig = 0.06\n",
    "I_sig = 0.01\n",
    "\n",
    "n_count_tri = np.zeros((3,3,3))\n",
    "p_ave_tri = np.zeros((3,3,3,3))\n",
    "\n",
    "def cond_test(val,base,sig):\n",
    "    \n",
    "    conds = [val < base-sig,val > base-sig and val < base+sig,val > base+sig]\n",
    "    \n",
    "    return conds.index(True)-1\n",
    "        \n",
    "T_ind = []\n",
    "C_ind = []\n",
    "P_ind = []\n",
    "I_ind = []\n",
    "for ind in range(0,n_data):\n",
    "    T_ind.append(cond_test(temp[ind],T_base,T_sig))\n",
    "    C_ind.append(cond_test(cloud[ind],C_base,C_sig))\n",
    "    P_ind.append(cond_test(precip[ind],P_base,P_sig))\n",
    "    I_ind.append(cond_test(icream[ind],I_base,I_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_data = np.asarray([T_ind,C_ind,P_ind,I_ind]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tot_data[0:5,:])\n",
    "print(type(tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
