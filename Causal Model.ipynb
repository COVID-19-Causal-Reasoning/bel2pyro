{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.integrate import odeint\n",
    "import networkx as nx\n",
    "\n",
    "import scipy.stats as sp_s\n",
    "\n",
    "import pybel as pb\n",
    "\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Causal Graph node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generic discrete probability function\n",
    "class cg_node():\n",
    "    def __init__(self,n_inputs,name):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.name = name\n",
    "        \n",
    "        if n_inputs == 0:\n",
    "            self.label = 'exogenous'\n",
    "        else:\n",
    "            self.label = 'endogenous'\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def p_init(self,input_data,var_data):\n",
    "        \n",
    "        self.n_data = len(input_data)\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.var_data = var_data\n",
    "        \n",
    "        if self.n_inputs == 0:\n",
    "            p_ave = np.zeros(3)\n",
    "            n_count = self.n_data\n",
    "            for i in range(0,3):\n",
    "                p_ave[i] = np.sum(var_data == i-1)/n_count\n",
    "        \n",
    "        elif self.n_inputs == 1:\n",
    "            n_count = np.zeros(3)\n",
    "            p_ave = np.zeros((3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                n_count[i] = np.sum(input_data == i-1)\n",
    "                for j in range(0,3):\n",
    "                    p_ave[j,i] = np.sum((input_data[:,0] == i-1)*(var_data == j-1))/n_count[i]\n",
    "            \n",
    "            \n",
    "        elif self.n_inputs == 2:\n",
    "            n_count = np.zeros((3,3))\n",
    "            p_ave = np.zeros((3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    n_count[i,j] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1))\n",
    "                    for k in range(0,3):\n",
    "                        p_ave[k,i,j] = np.sum(\n",
    "                            (input_data[:,0] == i-1)*(input_data[:,1] == j-1)*(var_data == k-1))/n_count[i,j]\n",
    "                        \n",
    "        elif self.n_inputs == 3:\n",
    "            n_count = np.zeros((3,3,3))\n",
    "            p_ave = np.zeros((3,3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    for k in range(0,3):\n",
    "                        n_count[i,j,k] = np.sum(\n",
    "                            (input_data[:,0] == i-1)*(input_data[:,1] == j-1)*(input_data[:,2] == k-1))\n",
    "                        for m in range(0,3):\n",
    "                            p_ave[m,i,j,k] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                *(input_data[:,2] == k-1)*(var_data == m-1))/n_count[i,j,k]\n",
    "                            \n",
    "        elif self.n_inputs == 4:\n",
    "            n_count = np.zeros((3,3,3,3))\n",
    "            p_ave = np.zeros((3,3,3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    for k in range(0,3):\n",
    "                        for m in range(0,3):\n",
    "                            n_count[i,j,k,m] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                *(input_data[:,2] == k-1)*(input_data[:,3] == m-1))\n",
    "                            for q in range(0,3):\n",
    "                                p_ave[q,i,j,k,m] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                    *(input_data[:,2] == k-1)*(input_data[:,3] == m-1)\n",
    "                                    *(var_data == q-1))/n_count[i,j,k,m]\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            print('error -- too many inputs')\n",
    "            return\n",
    "            \n",
    "        self.n_count = torch.tensor(n_count/self.n_data)\n",
    "        self.prob_dist = torch.tensor(p_ave)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sample(self,data_in=[]):\n",
    "        \n",
    "        if self.n_inputs == 0:\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = self.prob_dist)).bool()\n",
    "        elif self.n_inputs == 1:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0]])\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = p_temp)).bool()\n",
    "        elif self.n_inputs == 2:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0],data_in[1]])\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = p_temp)).bool()\n",
    "        elif self.n_inputs == 3:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0],data_in[1],data_in[2]])\n",
    "            samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = p_temp)).bool()\n",
    "        else:\n",
    "            print('error -- too many inputs')\n",
    "            samp_out = []\n",
    "        \n",
    "        return samp_out\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal graph class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cg_graph():\n",
    "    \n",
    "    def __init__(self,str_list=[],bel_graph=[]):\n",
    "        \n",
    "        edge_list = []\n",
    "\n",
    "        entity_list = []\n",
    "        \n",
    "        if str_list:\n",
    "\n",
    "            for item in str_list:\n",
    "\n",
    "                sub_ind = item.find('=')\n",
    "\n",
    "                sub_temp = item[:sub_ind-1]\n",
    "                obj_temp = item[sub_ind+3:]\n",
    "                \n",
    "                rel_temp = item[sub_ind:sub_ind+2]\n",
    "\n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "\n",
    "                edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "        elif bel_graph:\n",
    "            \n",
    "            for item in bel_graph.edges:\n",
    "                edge_temp = bel_graph.get_edge_data(item[0],item[1],item[2])\n",
    "                sub_temp = str(item[0])\n",
    "                obj_temp = str(item[1])\n",
    "                rel_temp = edge_temp['relation']\n",
    "                \n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "\n",
    "        n_nodes = len(entity_list)\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        adj_mat = np.zeros((n_nodes,n_nodes),dtype=int)\n",
    "\n",
    "        for item in edge_list:\n",
    "            out_ind = entity_list.index(item[0])\n",
    "            in_ind = entity_list.index(item[1])\n",
    "            adj_mat[out_ind,in_ind] = 1\n",
    "            \n",
    "        self.edge_list = edge_list\n",
    "        self.entity_list = entity_list\n",
    "        self.adj_mat = adj_mat\n",
    "        \n",
    "        self.graph = nx.DiGraph(adj_mat)\n",
    "        \n",
    "        node_dict = {}\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            node_dict[entity_list[i]] = cg_node(np.sum(adj_mat[:,i]),entity_list[i])\n",
    "        \n",
    "        self.node_dict = node_dict\n",
    "        \n",
    "        self.cond_list = []\n",
    "        \n",
    "        self.sample_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = []\n",
    "        self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(self.adj_mat[:,i] > 0)[0] for i in range(0,n_nodes)]\n",
    "        self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,n_nodes)]\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            self.parent_name_dict[entity_list[i]] = [entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            self.child_name_dict[entity_list[i]] = [entity_list[item] for item in self.child_ind_list[i]]\n",
    "\n",
    "        # create rank-3 delta tensor\n",
    "        tensor_temp = torch.zeros((3,3,3)).double()\n",
    "        for i in range(0,3):\n",
    "            tensor_temp[i,i,i] = 1\n",
    "            \n",
    "        self.tensor_temp = tensor_temp\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def prob_init(self,data_in):\n",
    "        # initialize all of the nodes\n",
    "        \n",
    "        exog_list = []\n",
    "        prob_dict = {}\n",
    "        \n",
    "        for name in self.node_dict:\n",
    "            i = self.entity_list.index(name)\n",
    "            data_in_temp = data_in[:,self.parent_ind_list[i]]\n",
    "            data_out_temp = data_in[:,i]\n",
    "            \n",
    "            self.node_dict[name].p_init(data_in_temp,data_out_temp)\n",
    "            \n",
    "            if self.node_dict[name].n_inputs == 0:\n",
    "                exog_list.append(name)\n",
    "            prob_dict[name] = self.node_dict[name].prob_dist\n",
    "        \n",
    "        self.exog_list = exog_list\n",
    "        self.prob_dict = prob_dict\n",
    "\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def sample_vars(self,names,flag=0):\n",
    "        # do a multi-variable sample\n",
    "        \n",
    "        # sample only those variables w/o sample data\n",
    "        if np.any([item in self.sample_dict for item in names]):\n",
    "            sample_list = [item for item in names if item not in self.sample_dict]\n",
    "            if sample_list:\n",
    "                self.sample_vars(sample_list,flag+1)\n",
    "        \n",
    "        # sample exogenous variables\n",
    "        elif np.any([item in self.exog_list for item in names]):\n",
    "            in_exog = [item for item in names if item in self.exog_list]\n",
    "            not_in_exog = [item for item in names if item not in self.exog_list]\n",
    "            \n",
    "            for item in in_exog:\n",
    "                self.sample_dict[item] = self.node_dict[item].sample()\n",
    "            if not_in_exog:              \n",
    "                self.sample_vars(not_in_exog,flag+1)\n",
    "            \n",
    "        # if you have samples from all of the parents, sample names\n",
    "        # otherwise, sample the parents\n",
    "        elif names:\n",
    "            \n",
    "            sample_list = []\n",
    "            sample_list2 = []\n",
    "            \n",
    "            for item in names:\n",
    "                parent_list = self.parent_name_dict[item]\n",
    "                \n",
    "                if np.all([item2 in self.sample_dict for item2 in parent_list]):\n",
    "                    self.sample_dict[item] = self.node_dict[item].sample(\n",
    "                        [self.sample_dict[item2] for item2 in parent_list])\n",
    "                else:\n",
    "                    sample_list = sample_list + [item2 for item2 in parent_list if item2 not in sample_list]\n",
    "                    sample_list2 = sample_list2 + [item]\n",
    "            if sample_list:\n",
    "                self.sample_vars(sample_list,flag+1)\n",
    "            if sample_list2:\n",
    "                self.sample_vars(sample_list2,flag+1)\n",
    "            \n",
    "        # if you're back at the root node, return the samples\n",
    "        # otherwise, don't return anything - the values are stored in self.sample_dict\n",
    "        if flag == 0:\n",
    "            \n",
    "            tensor_sample = torch.Tensor([-1,0,1]).int()\n",
    "            \n",
    "            output =[tensor_sample[self.sample_dict[item]] for item in names]\n",
    "            self.sample_dict = {}\n",
    "            return output\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "\n",
    "    def gen_path_nodes(self,sources,destinations):\n",
    "        \n",
    "        source_inds = [self.entity_list.index(item) for item in sources]\n",
    "        dest_inds = [self.entity_list.index(item) for item in destinations]\n",
    "        \n",
    "        nodes = []\n",
    "        \n",
    "        for i in source_inds:\n",
    "            for j in dest_inds:      \n",
    "                for path in nx.all_simple_paths(self.graph, source=i, target=j):\n",
    "                    for ind in path:\n",
    "                        if self.entity_list[ind] not in nodes:\n",
    "                            nodes.append(self.entity_list[ind])\n",
    "                            \n",
    "        return nodes\n",
    "        \n",
    "    \n",
    "    def joint_dist_add(self,add_node,nodes_temp,prob_temp):\n",
    "        # create a new joint distribution with add_node now included\n",
    "        \n",
    "        # deliberately let out 'a' - for the variable being added\n",
    "        str_temp = 'bcdefghijklmnopqrstuvwxyz'\n",
    "        \n",
    "        # find parent indices\n",
    "        par_inds = [nodes_temp.index(item2) for item2 in self.parent_name_dict[add_node]]\n",
    "\n",
    "        n_inds = len(par_inds)\n",
    "\n",
    "        if n_inds == 1:\n",
    "            str1 = 'ay,' + str_temp[par_inds[0]] + 'yz,'\n",
    "            str2 = str_temp[:len(nodes_temp)].replace(str_temp[par_inds[0]],'z')\n",
    "\n",
    "            str_sum = str1 + str2\n",
    "            prob_out = torch.einsum(str_sum,self.prob_dict[add_node],self.tensor_temp,prob_temp)\n",
    "\n",
    "        elif n_inds == 2:                        \n",
    "            str1 = 'awy,' + str_temp[par_inds[0]] + 'wx,' + str_temp[par_inds[1]] + 'yz,'\n",
    "\n",
    "            str2 = str_temp[:len(nodes_temp)].replace(\n",
    "                str_temp[par_inds[0]],'x').replace(str_temp[par_inds[1]],'z')\n",
    "\n",
    "            str_sum = str1 + str2\n",
    "            prob_out = torch.einsum(str_sum,self.prob_dict[add_node],self.tensor_temp,self.tensor_temp,prob_temp)\n",
    "\n",
    "        elif n_inds == 3:\n",
    "            str1 = ('auwy,' + str_temp[par_inds[0]] + 'uv,' + str_temp[par_inds[1]] \n",
    "                    + 'wx,' + str_temp[par_inds[2]] + 'yz,')\n",
    "\n",
    "            str2 = str_temp[:len(nodes_temp)].replace(\n",
    "                str_temp[par_inds[0]],'u').replace(str_temp[par_inds[1]],'x').replace(\n",
    "                str_temp[par_inds[2]],'z')\n",
    "\n",
    "            str_sum = str1 + str2\n",
    "            prob_out = torch.einsum(str_sum,\n",
    "                self.prob_dict[add_node],self.tensor_temp,self.tensor_temp,self.tensor_temp,prob_temp)\n",
    "            \n",
    "        else:\n",
    "            print('too many parents')\n",
    "            prob_out = prob_temp\n",
    "            \n",
    "        return prob_out\n",
    "        \n",
    "    \n",
    "    def calc_prob(self,names):\n",
    "        # calculate the joint probability over a list of named nodes\n",
    "        \n",
    "        # find all paths from exogenous nodes        \n",
    "        path_nodes = self.gen_path_nodes(self.exog_list,names)\n",
    "        \n",
    "        for item in names:\n",
    "            if item in self.exog_list and item not in path_nodes:\n",
    "                path_nodes.append(item)\n",
    "        \n",
    "        print(path_nodes)\n",
    "\n",
    "        # get joint exogenous probability distribution\n",
    "        nodes_temp = []\n",
    "        for item in self.exog_list:\n",
    "            if item in path_nodes:\n",
    "                nodes_temp.append(item)\n",
    "                \n",
    "        #print(nodes_temp)\n",
    "        #print(self.exog_list)\n",
    "        prob_temp = self.prob_dict[nodes_temp[0]]\n",
    "        for item in nodes_temp[1:]:\n",
    "            prob_temp = torch.einsum('...i,j',prob_temp,self.prob_dict[item])\n",
    "        \n",
    "        # identify all of the children of nodes_temp in path_nodes\n",
    "        child_nodes = []\n",
    "        for item in nodes_temp:\n",
    "            for item2 in self.child_name_dict[item]:\n",
    "                if item2 in path_nodes and item2 not in child_nodes:\n",
    "                    child_nodes.append(item2)\n",
    "            \n",
    "        flag = 0\n",
    "        \n",
    "        # iterate through node children until target nodes are reached\n",
    "        while flag == 0:\n",
    "            #print(nodes_temp)\n",
    "            \n",
    "            # determine which nodes to add\n",
    "            # all children of nodes_temp in path_nodes, not in nodes_temp, and that have all their parents in\n",
    "            # nodes_temp\n",
    "            add_nodes = []\n",
    "            for item in nodes_temp:\n",
    "                for item2 in self.child_name_dict[item]:\n",
    "                    if (item2 in path_nodes\n",
    "                        and item2 not in nodes_temp\n",
    "                        and np.all([item3 in nodes_temp for item3 in self.parent_name_dict[item2]])\n",
    "                        and item2 not in add_nodes):\n",
    "                        add_nodes.append(item2)\n",
    "            \n",
    "            #print(add_nodes)\n",
    "\n",
    "            # add nodes to the joint distribution\n",
    "            for item in add_nodes:\n",
    "                prob_temp = self.joint_dist_add(item,nodes_temp,prob_temp)\n",
    "\n",
    "                # add the new node to nodes_temp\n",
    "                nodes_temp = [item] + nodes_temp\n",
    "                \n",
    "            # determine which nodes to subtract\n",
    "            # all nodes in nodes_temp not in names and that have all their children in nodes_temp\n",
    "            sub_nodes = []\n",
    "            for item in nodes_temp:\n",
    "                \n",
    "                child_path_list = []\n",
    "                \n",
    "                for item2 in self.child_name_dict[item]:\n",
    "                    if item2 in path_nodes:\n",
    "                        child_path_list.append(item2)\n",
    "\n",
    "                if item not in names and np.all([item2 in nodes_temp for item2 in child_path_list]):\n",
    "                    sub_nodes.append(item)\n",
    "                    \n",
    "            #print(sub_nodes)\n",
    "                    \n",
    "            # sum over the sub_nodes probabilities\n",
    "            \n",
    "            if sub_nodes:\n",
    "                remove_indices = [nodes_temp.index(item) for item in sub_nodes]\n",
    "                prob_temp = torch.sum(prob_temp,dim=remove_indices)\n",
    "            \n",
    "                # remove summed nodes from nodes_temp\n",
    "                for item in sub_nodes:\n",
    "                    nodes_temp.remove(item)\n",
    "                    \n",
    "            if sorted(nodes_temp) == sorted(names):\n",
    "                flag = 1\n",
    "                permute_inds = [nodes_temp.index(item) for item in names] \n",
    "                prob_temp = prob_temp.permute(permute_inds)\n",
    "\n",
    "            #print()\n",
    "            \n",
    "            \n",
    "        return prob_temp\n",
    "    \n",
    "    \n",
    "    def calc_cond_prob(self,cond_prob,uncond_prob):\n",
    "        \n",
    "        # check to make sure the lists don't overlap\n",
    "        if np.any([item in uncond_prob for item in cond_prob]):\n",
    "            print('error -- overlapping lists')\n",
    "            return\n",
    "        \n",
    "        n_cond = len(cond_prob)\n",
    "        n_uncond = len(cond_prob)\n",
    "        \n",
    "        p_joint = self.calc_prob(cond_prob + uncond_prob)\n",
    "        p_uncond = self.calc_prob(uncond_prob)\n",
    "                \n",
    "        if n_uncond == 1:\n",
    "            p_cond = torch.einsum('...i,ijk,j',p_joint,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 2:\n",
    "            p_cond = torch.einsum('...il,ijk,lmn,jm',p_joint,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 3:\n",
    "            p_cond = torch.einsum('...ilp,ijk,lmn,pqr,jmq',\n",
    "                p_joint,self.tensor_temp,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        else:\n",
    "            print('too many conditioned variables')\n",
    "            p_cond = p_joint\n",
    "            \n",
    "        return p_cond\n",
    "            \n",
    "    \n",
    "    def calc_do(self,names,do_vars,do_vals):\n",
    "        # calculate the final probability distribution of the variable in question given do variables\n",
    "        \n",
    "        # add do_vars to list of exogenous variables\n",
    "        self.exog_list += do_vars\n",
    "        \n",
    "        # sever links from do_vars to parents\n",
    "        child_temp = self.child_name_dict\n",
    "        \n",
    "        for item in self.child_name_dict:\n",
    "            for item2 in do_vars:\n",
    "                if item2 in self.child_name_dict[item]:\n",
    "                    self.child_name_dict[item].remove(item2)\n",
    "        \n",
    "        names_prob_dict = {}\n",
    "        for item in do_vars:\n",
    "            names_prob_dict[item] = self.prob_dict[item]\n",
    "        \n",
    "        # specify distributions for those do_vars\n",
    "        for item in do_vars:\n",
    "            p_temp = np.zeros(3)\n",
    "            p_temp[do_vals[item]+1] = 1\n",
    "            self.prob_dict[item] = torch.Tensor(p_temp)\n",
    "            \n",
    "        \n",
    "        prob_out = self.calc_prob(names)\n",
    "        \n",
    "        # restore original list of exogenous variables\n",
    "        for item in do_vars:\n",
    "            self.exog_list.remove(item)\n",
    "        \n",
    "        # restore original probability distributions\n",
    "        for item in do_vars:\n",
    "            self.prob_dict[item] = names_prob_dict[item]\n",
    "        \n",
    "        # restore original child dictionary\n",
    "        self.child_name_dict = child_temp        \n",
    "        \n",
    "        return prob_out\n",
    "    \n",
    "        \n",
    "    def calc_do_cond(self,do_vars,do_vals,cond_prob,uncond_prob):\n",
    "        \n",
    "        # check to make sure the lists don't overlap\n",
    "        if (np.any([item in uncond_prob for item in cond_prob]) \n",
    "            or np.any([item in do_vars for item in cond_prob])\n",
    "            or np.any([item in do_vars for item in uncond_prob])):\n",
    "            print('error -- overlapping lists')\n",
    "            return\n",
    "        \n",
    "        n_cond = len(cond_prob)\n",
    "        n_uncond = len(cond_prob)\n",
    "            \n",
    "        \n",
    "        p_joint = self.calc_do(cond_prob + uncond_prob,do_vars,do_vals)\n",
    "        p_uncond = self.calc_do(uncond_prob,do_vars,do_vals)\n",
    "                \n",
    "        if n_uncond == 1:\n",
    "            p_cond = torch.einsum('...i,ijk,j',p_joint,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 2:\n",
    "            p_cond = torch.einsum('...il,ijk,lmn,jm',p_joint,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        elif n_uncond == 3:\n",
    "            p_cond = torch.einsum('...ilp,ijk,lmn,pqr,jmq',\n",
    "                p_joint,self.tensor_temp,self.tensor_temp,self.tensor_temp,1/p_uncond)\n",
    "        else:\n",
    "            print('too many conditioned variables')\n",
    "            p_cond = p_joint\n",
    "        \n",
    "        return p_cond\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    def calc_counterfact():\n",
    "        return\n",
    "    \n",
    "    def calc_cde(self,names,do_vars,do_vals,ctrl_vars,ctrl_vals):\n",
    "        tot_vars = do_vars + ctrl_vars\n",
    "        tot_vals = {}\n",
    "        tot_ctrl_vals = {}\n",
    "        for item in do_vars:\n",
    "            tot_vals[item] = do_vals[item]\n",
    "            tot_ctrl_vals = ctrl_vals[item]\n",
    "            \n",
    "        for item in ctrl_vars:\n",
    "            tot_vals[item] = 0\n",
    "            tot_ctrl_vals[item] = ctrl_vals[item]\n",
    "            \n",
    "        return self.calc_do(names,tot_vars,tot_vals) - self.calc_do(names,tot_vars,tot_ctrl_vals)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def calc_te(self,names,do_vars,do_vals):\n",
    "        \n",
    "        do_vals_0 = {}\n",
    "        for item in do_vals:\n",
    "            do_vals_0[item] = 0\n",
    "            \n",
    "        return self.calc_do(names,do_vars,do_vals) - self.calc_do(do_vars,do_vals_0)\n",
    "    \n",
    "    def calc_nde(self,names,do_vars,do_vals):\n",
    "        \n",
    "        # identify parents of names\n",
    "        parent_list = []\n",
    "        for item in names:\n",
    "            for item2 in self.parent_name_dict[item]:\n",
    "                if item2 not in parent_list:\n",
    "                    parent_list.append(item2)\n",
    "                    \n",
    "        par_do_vals = []\n",
    "        for item in do_vars:\n",
    "            if item in parent_list:\n",
    "                par_do_vals.append(item)\n",
    "        \n",
    "        if not par_do_vals:\n",
    "            print('no direct effect')\n",
    "            prob_out = 0\n",
    "        \n",
    "        else:\n",
    "            # calculate probability of non-do_var parents given do_vars = 0\n",
    "            \n",
    "            non_do_parents = [item for item in parent_list if item not in do_vars]\n",
    "            do_vals_0 = {}\n",
    "            for item in do_vars:\n",
    "                do_vals_0[item] = 0\n",
    "            \n",
    "            non_par_do_vars = []\n",
    "            for item in do_vars:\n",
    "                if item not in parent_list:\n",
    "                    non_par_do_vars.append(item)\n",
    "            \n",
    "            nodes_temp = non_do_parents\n",
    "            \n",
    "            prob_temp = self.calc_do(non_do_parents,do_vars,do_vals_0)\n",
    "            # do outer product to get overall distribution\n",
    "            for item in non_par_do_vars:\n",
    "                p_temp = np.zeros(3)\n",
    "                p_temp[do_vals[item]] = 1\n",
    "                p_add = torch.Tensor(p_temp)\n",
    "                prob_temp = torch.einsum('...i,j',prob_temp,p_add)\n",
    "                nodes_temp.append(item)\n",
    "            \n",
    "            n_sum = len(nodes_temp)\n",
    "            n_names = len(names)\n",
    "            for item in names:\n",
    "                prob_temp = self.joint_dist_add(item,nodes_temp,prob_temp)\n",
    "                \n",
    "            # calculate overall joint probability distribution\n",
    "            sum_axes = range(n_names,n_names+n_sum)\n",
    "            prob_do_vals = np.sum(prob_temp,axis=tuple(sum_axes))\n",
    "            \n",
    "            prob_out = prob_do_vals - self.calc_do(names,do_vars,do_vals_0)\n",
    "\n",
    "        return prob_out\n",
    "    \n",
    "    def calc_nie():\n",
    "        # identify parents of names\n",
    "        parent_list = []\n",
    "        for item in names:\n",
    "            for item2 in self.parent_name_dict[item]:\n",
    "                if item2 not in parent_list:\n",
    "                    parent_list.append(item2)\n",
    "                    \n",
    "        par_do_vals = []\n",
    "        for item in parent_list:\n",
    "            if item in do_vars:\n",
    "                par_do_vals.append(item)\n",
    "        \n",
    "        if np.all([item in do_vars for item in parent_list]):\n",
    "            print('no indirect effect')\n",
    "            prob_out = 0\n",
    "        \n",
    "        else:\n",
    "            # calculate probability of non-do_var parents given do_vars = do_vals\n",
    "            \n",
    "            non_do_parents = [item for item in parent_list if item not in do_vars]\n",
    "            do_vals_0 = {}\n",
    "            for item in do_vars:\n",
    "                do_vals_0[item] = 0\n",
    "            \n",
    "            non_par_do_vars = []\n",
    "            for item in do_vars:\n",
    "                if item not in parent_list:\n",
    "                    non_par_do_vars.append(item)\n",
    "            \n",
    "            nodes_temp = non_do_parents\n",
    "            \n",
    "            prob_temp = self.calc_do(non_do_parents,do_vars,do_vals)\n",
    "            # do outer product to get overall distribution\n",
    "            for item in non_par_do_vars:\n",
    "                p_temp = np.zeros(3)\n",
    "                p_temp[1] = 1\n",
    "                p_add = torch.Tensor(p_temp)\n",
    "                prob_temp = torch.einsum('...i,j',prob_temp,p_add)\n",
    "                nodes_temp.append(item)\n",
    "            \n",
    "            n_sum = len(nodes_temp)\n",
    "            n_names = len(names)\n",
    "            for item in names:\n",
    "                prob_temp = self.joint_dist_add(item,nodes_temp,prob_temp)\n",
    "                \n",
    "            # calculate overall joint probability distribution\n",
    "            sum_axes = range(n_names,n_names+n_sum)\n",
    "            prob_do_vals = np.sum(prob_temp,axis=tuple(sum_axes))\n",
    "            \n",
    "            prob_out = prob_do_vals - self.calc_do(names,do_vars,do_vals_0)\n",
    "\n",
    "        return prob_out\n",
    "\n",
    "    def cond_mut_info(self,target,test,cond,data_in):\n",
    "        \n",
    "        cond_temp = cond\n",
    "        \n",
    "        if not cond:\n",
    "            # find parents of target\n",
    "            for item in target:\n",
    "                for item2 in self.parent_name_dict[item]:\n",
    "                    if item2 not in cond_temp:\n",
    "                        cond_temp.append(item2)\n",
    "        \n",
    "        \n",
    "        target_inds = [self.entity_list.index(item) for item in target]\n",
    "        test_inds = [self.entity_list.index(item) for item in test]\n",
    "        cond_inds = [self.entity_list.index(item) for item in cond_temp]\n",
    "        \n",
    "        n_total = len(data_in)\n",
    "        \n",
    "        null_joint = data_in[:,target_inds + cond_inds]\n",
    "        null_cond = data_in[:,cond_inds]\n",
    "        \n",
    "        hypth_joint = data_in[:,target_inds + test_inds + cond_inds]\n",
    "        hypth_cond = data_in[:,test_inds + cond_inds]\n",
    "        \n",
    "        null_entropy = 0\n",
    "        null_list = []\n",
    "        \n",
    "        hypth_entropy = 0\n",
    "        hypth_list = []\n",
    "        for i in range(0,n_total):\n",
    "\n",
    "            if np.all([np.any(null_joint[i,:] != item) for item in null_list]):\n",
    "                num_sum = np.sum([np.all(null_joint[i,:] == item) for item in null_joint])\n",
    "                denom_sum = np.sum([np.all(null_cond[i,:] == item) for item in null_cond])\n",
    "                null_entropy = null_entropy - num_sum*np.log(num_sum/denom_sum)\n",
    "                null_list.append(null_joint[i,:])\n",
    "            \n",
    "            if np.all([np.any(hypth_joint[i,:] != item) for item in hypth_list]):\n",
    "                num_sum = np.sum([np.all(hypth_joint[i,:] == item) for item in hypth_joint])\n",
    "                denom_sum = np.sum([np.all(hypth_cond[i,:] == item) for item in hypth_cond])\n",
    "                hypth_entropy = hypth_entropy - num_sum*np.log(num_sum/denom_sum)\n",
    "                hypth_list.append(hypth_joint[i,:])\n",
    "                \n",
    "        return (null_entropy - hypth_entropy)/n_total\n",
    "        \n",
    "    def g_test(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest\n",
    "        \n",
    "        p_name = self.calc_prob(name)*len(data_in)\n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        name_data = data_in[:,name_ind]\n",
    "        \n",
    "        p_data = torch.Tensor([np.sum(name_data == -1),np.sum(name_data == 0),np.sum(name_data == 1)])\n",
    "        \n",
    "        print(p_name)\n",
    "        print(p_data)\n",
    "        \n",
    "        g_val = 2*torch.sum(p_data*torch.log(p_data/p_name))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "    \n",
    "    def g_test_emp(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest\n",
    "        \n",
    "        #p_name = self.calc_prob(name)*len(data_in)\n",
    "        # generate an empirical distribution for variable name\n",
    "        model_data = np.zeros(len(data_in))\n",
    "        for i in range(0,len(data_in)):\n",
    "            model_data[i] = self.sample_vars(name)[0].item()\n",
    "            \n",
    "        p_model = torch.Tensor([np.sum(model_data == -1),np.sum(model_data == 0),np.sum(model_data == 1)])\n",
    "        print(p_model)\n",
    "        \n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        name_data = data_in[:,name_ind]\n",
    "        \n",
    "        p_data = torch.Tensor([np.sum(name_data == -1),np.sum(name_data == 0),np.sum(name_data == 1)])\n",
    "        print(p_data)\n",
    "        \n",
    "        g_val = 2*torch.sum(p_data*torch.log(p_data/p_model))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "    \n",
    "    def write_to_cf(self,filename):\n",
    "        # write the causal graph to a text file to import into causal fusion\n",
    "        \n",
    "        pos_dict = nx.drawing.layout.planar_layout(self.graph)\n",
    "        node_filler1 = (',\"label\":\"\",\"shape\":\"ellipse\",\"fontSize\":14,\"sizeLabelMode\":5,\"font\":{\"size\":14}'\n",
    "            + ',\"size\":14,\"labelNodeId\":\"node')\n",
    "        \n",
    "        \n",
    "        node_filler2 = ('ID\",\"labelNodeOffset\":{\"x\":0,\"y\":0},'\n",
    "            + '\"labelOffset\":{\"x\":0,\"y\":0},\"shadow\":{\"color\":\"#00000080\",\"size\":0,\"x\":0,\"y\":0}}},')\n",
    "        \n",
    "        edge_filler = '\",\"type\":\"directed\",\"metadata\":{\"isLabelDraggable\":true,\"label\":\"\"}},'\n",
    "        \n",
    "        txt_file = open(filename + '.txt','w',newline='')\n",
    "        \n",
    "        str_list = []\n",
    "        \n",
    "        str_list.append('{\"name\":\"bel_graph\",')\n",
    "        str_list.append('\"nodes\":[')\n",
    "        \n",
    "        # write nodes\n",
    "        for i in range(0,len(self.entity_list)):\n",
    "            name = self.entity_list[i]\n",
    "            str_temp = ('{\"id\":\"node' + str(i) + '\",\"name\":\"' + name + '\",\"label\":\"' + name \n",
    "                + '\",\"type\":\"basic\",\"metadata\":{\"x\":' + str(pos_dict[i][0]*100) + ',\"y\":' \n",
    "                + str(pos_dict[i][1]*100))\n",
    "            \n",
    "            if i == len(self.entity_list) - 1:\n",
    "                str_list.append(str_temp + node_filler1 + str(i) + node_filler2[:-1])\n",
    "            else:\n",
    "                str_list.append(str_temp + node_filler1 + str(i) + node_filler2)\n",
    "            \n",
    "        \n",
    "        str_list.append('],')\n",
    "        \n",
    "        str_list.append('\"edges\":[')\n",
    "        \n",
    "        # write edges\n",
    "        for i in range(0,len(self.edge_list)):\n",
    "            item = self.edge_list[i]\n",
    "            from_node = self.entity_list.index(item[0])\n",
    "            to_node = self.entity_list.index(item[1])\n",
    "            \n",
    "            str_temp = ('{\"id\":\"node' + str(from_node) + '->node' + str(to_node) + '\",\"from\":\"'\n",
    "                + item[0] + '\",\"to\",\"' + item[1])\n",
    "            \n",
    "            if i == len(self.edge_list) - 1:\n",
    "                str_list.append(str_temp + edge_filler[:-1])\n",
    "            else:\n",
    "                str_list.append(str_temp + edge_filler)\n",
    "                        \n",
    "        \n",
    "        str_list.append('],')\n",
    "        str_list.append('\"task\":{}')\n",
    "        str_list.append('\"metadata\":{},')\n",
    "        \n",
    "        str_list.append('\"project_id\":\"88D621AF72E0634C\",\"_fileType\":\"graph\"}')\n",
    "        \n",
    "        \n",
    "        [txt_file.writelines(item) for item in str_list]\n",
    "        \n",
    "        txt_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out cg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bel_temp = pb.from_bel_script('temp.txt')\n",
    "graph_test = cg_graph(bel_graph=bel_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'adj_mat', 'calc_cde', 'calc_cond_prob', 'calc_counterfact', 'calc_do', 'calc_do_cond', 'calc_nde', 'calc_nie', 'calc_prob', 'calc_te', 'child_ind_list', 'child_name_dict', 'cond_list', 'cond_mut_info', 'edge_list', 'entity_list', 'g_test', 'g_test_emp', 'gen_path_nodes', 'graph', 'joint_dist_add', 'n_nodes', 'node_dict', 'parent_ind_list', 'parent_name_dict', 'prob_init', 'sample_dict', 'sample_vars', 'tensor_temp', 'write_to_cf']\n",
      "\n",
      "['complex(a(CHEBI:26667 ! \"sialic acid\"), p(HGNC:1659 ! CD33))', 'p(HGNC:1659 ! CD33)', 'increases']\n",
      "\n",
      "['a(CHEBI:26667 ! \"sialic acid\")', 'complex(a(CHEBI:26667 ! \"sialic acid\"), p(HGNC:1659 ! CD33))', 'partOf']\n",
      "\n",
      "['a(CHEBI:26667 ! \"sialic acid\")', 'complex(a(CHEBI:26667 ! \"sialic acid\"), p(HGNC:1659 ! CD33))', 'partOf']\n",
      "\n",
      "['p(HGNC:1659 ! CD33)', 'complex(a(CHEBI:26667 ! \"sialic acid\"), p(HGNC:1659 ! CD33))', 'partOf']\n",
      "\n",
      "['p(HGNC:1659 ! CD33)', 'complex(a(CHEBI:26667 ! \"sialic acid\"), p(HGNC:1659 ! CD33))', 'partOf']\n",
      "\n",
      "['p(HGNC:1659 ! CD33)', 'p(HGNC:1659 ! CD33, pmod(Ph))', 'hasVariant']\n",
      "\n",
      "['p(HGNC:1659 ! CD33)', 'p(HGNC:1659 ! CD33, pmod(Ph))', 'increases']\n",
      "\n",
      "['p(HGNC:1659 ! CD33, pmod(Ph))', 'p(HGNC:9658 ! PTPN6)', 'directlyIncreases']\n",
      "\n",
      "['p(HGNC:1659 ! CD33, pmod(Ph))', 'p(HGNC:9644 ! PTPN11)', 'directlyIncreases']\n",
      "\n",
      "['p(HGNC:9658 ! PTPN6)', 'p(HGNC:11491 ! SYK)', 'directlyDecreases']\n",
      "\n",
      "['p(HGNC:9644 ! PTPN11)', 'p(HGNC:11491 ! SYK)', 'directlyDecreases']\n",
      "\n",
      "['p(HGNC:11491 ! SYK)', 'p(HGNC:17761 ! TREM2)', 'increases']\n",
      "\n",
      "['p(HGNC:11491 ! SYK)', 'p(HGNC:12449 ! TYROBP)', 'increases']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dir(graph_test))\n",
    "print()\n",
    "for item in graph_test.edge_list:\n",
    "    print(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zuck016/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/networkx/drawing/layout.py:923: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  pos = np.row_stack((pos[x] for x in node_list))\n",
      "/Users/zuck016/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n",
      "/Users/zuck016/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if cb.iterable(node_size):  # many node sizes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-0.90909091 -0.34848485]\n",
      "1\n",
      "[ 1.         -0.34848485]\n",
      "2\n",
      "[-0.63636364 -0.07575758]\n",
      "3\n",
      "[ 0.72727273 -0.07575758]\n",
      "4\n",
      "[-0.5        -0.21212121]\n",
      "5\n",
      "[0.18181818 0.46969697]\n",
      "6\n",
      "[-0.22727273  0.06060606]\n",
      "7\n",
      "[0.18181818 0.33333333]\n",
      "8\n",
      "[0.18181818 0.1969697 ]\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "nx.draw_planar(graph_test.graph)\n",
    "plt.show()\n",
    "\n",
    "pos_dict = nx.drawing.layout.planar_layout(graph_test.graph)\n",
    "for item in pos_dict:\n",
    "    print(item)\n",
    "    print(pos_dict[item])\n",
    "graph_test.write_to_cf('sag_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "str_list = ['temp =| cloudy','cloudy => rainy','temp => icream','rainy =| icream']\n",
    "graph_test = cg_graph(str_list=str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 77 (char 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9a3b25752e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sag_graph.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msag_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msag_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 77 (char 76)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('sag_graph.json') as sag_json:\n",
    "    sag = json.load(sag_json)\n",
    "sag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print([graph_test.node_dict[item].name,graph_test.node_dict[item].n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test.prob_init(tot_data)\n",
    "print(graph_test.exog_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(item)\n",
    "    print(graph_test.node_dict[item].n_count)\n",
    "    print(graph_test.node_dict[item].prob_dist)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test.sample_vars(['icream','rainy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph_test.calc_prob(['rainy','icream'])\n",
    "y = graph_test.calc_prob(['icream'])\n",
    "y2 = graph_test.calc_prob(['rainy'])\n",
    "print(x)\n",
    "print()\n",
    "print(y)\n",
    "print(torch.sum(x,dim=0))\n",
    "\n",
    "print()\n",
    "print(y2)\n",
    "print(torch.sum(x,dim=1))\n",
    "\n",
    "# this is somehow reversed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "z = graph_test.calc_cond_prob(['rainy'],['icream'])\n",
    "print()\n",
    "print(z)\n",
    "print()\n",
    "print(torch.matmul(z,y))\n",
    "\n",
    "print(torch.sum(z,dim=1))\n",
    "\n",
    "print(torch.sum(z,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "do_dict = {}\n",
    "do_dict['rainy'] = torch.Tensor([1]).int()\n",
    "for item in do_dict:\n",
    "    print(do_dict[item])\n",
    "    print(graph_test.prob_dict[item])\n",
    "\n",
    "\n",
    "print(graph_test.exog_list)\n",
    "a1 = graph_test.calc_do(['icream'],['rainy'],do_dict)\n",
    "print(a1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "graph_test.calc_do_cond(['rainy'],do_dict,['temp'],['icream'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for item in graph_test.prob_dict:\n",
    "    print(item)\n",
    "    print(graph_test.prob_dict[item])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_test.cond_mut_info(['rainy'],['temp'],['cloudy'],tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(graph_test.gen_path_nodes(graph_test.exog_list,['temp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_test.prob_dict['icream'])\n",
    "\n",
    "a = graph_test.g_test(['icream'],tot_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(graph_test.prob_dict['cloudy'][:,graph_test.node_dict['temp'].sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_test.prob_dict['cloudy'])\n",
    "print(graph_test.calc_prob(['cloudy']))\n",
    "print(torch.matmul(graph_test.prob_dict['cloudy'],graph_test.prob_dict['temp']))\n",
    "print(torch.matmul(graph_test.prob_dict['temp'],graph_test.prob_dict['cloudy']))\n",
    "print()\n",
    "\n",
    "a = graph_test.g_test_emp(['icream'],tot_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = pyro.sample('',pyro.distributions.Multinomial(probs = torch.Tensor([0.3,0.2,0.5]))).bool()\n",
    "#samp_out = pyro.sample('',pyro.distributions.Multinomial(probs = self.prob_dist))\n",
    "print(x)\n",
    "y = torch.Tensor([-1,0,1])[x]\n",
    "print(y)\n",
    "\n",
    "z = torch.Tensor([1]).int()\n",
    "print(z)\n",
    "print(z+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(graph_test.calc_prob(['icream']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(dir(graph_test))\n",
    "print()\n",
    "print(graph_test.entity_list)\n",
    "print(graph_test.adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sp.stats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def indep_vars(n_samples):\n",
    "    \n",
    "    T_list = []\n",
    "    C_list = []\n",
    "    P_list = []\n",
    "    \n",
    "    for i in range(0,n_samples):\n",
    "        \n",
    "        #x = pyro.sample(\"x_{}\".format(i), pyro.distributions.Normal(20,5))\n",
    "        \n",
    "        #T_temp = pyro.distributions.Normal(20,5).sample()\n",
    "        #C_temp = 0.5*pyro.distributions.Beta(1,1+T_temp/10).sample() + 0.5*pyro.distributions.Uniform(0,1).sample()\n",
    "        #P_temp = (0.5*pyro.distributions.Exponential(1).sample() \n",
    "            #+ 0.5*pyro.distributions.Exponential(1/(C_temp+1)).sample())\n",
    "        \n",
    "        T_list.append(pyro.sample(\"T_{}\".format(i), pyro.distributions.Normal(20,5)))\n",
    "        \n",
    "        C_list.append(0.5*pyro.sample(\"C1_{}\".format(i),pyro.distributions.Beta(1,1+T_list[-1]/10)) \n",
    "            + 0.5*pyro.sample(\"C2_{}\".format(i),pyro.distributions.Uniform(0,1)))\n",
    "        P_list.append(0.5*pyro.sample(\"P1_{}\".format(i), pyro.distributions.Exponential(1))\n",
    "            + 0.5*pyro.sample(\"P2.{}\".format(i),pyro.distributions.Exponential(1/(C_list[-1]+1))))\n",
    "        \n",
    "    return T_list,C_list,P_list\n",
    "\n",
    "def dep_vars(T_list,C_list,P_list):\n",
    "    \n",
    "    n_pts = len(T_list)\n",
    "    \n",
    "    I_list = []\n",
    "    \n",
    "    for i in range(0,n_pts):\n",
    "        \n",
    "        T_temp = T_list[i]\n",
    "        C_temp = C_list[i]\n",
    "        P_temp = P_list[i]\n",
    "        \n",
    "        if P_temp > 2.5 or T_temp < 15:\n",
    "            I_list.append(1e-6*pyro.sample(\"I_{}\".format(i),pyro.distributions.Bernoulli(1)))\n",
    "        else:\n",
    "            I_list.append(pyro.sample(\"I_{}\".format(i),\n",
    "                pyro.distributions.Beta(2*(2.5-P_temp)*(T_temp-12)/(2.5*12),2)))\n",
    "            #I_temp = torch.tensor(0.5)\n",
    "        \n",
    "    return I_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "temp,cloud,precip = indep_vars(n_data)\n",
    "icream = dep_vars(temp,cloud,precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# trinarize data relative to baseline\n",
    "T_base = 20\n",
    "C_base = 0.38\n",
    "P_base = 1.2\n",
    "I_base = 0.23\n",
    "\n",
    "T_sig = 1.0\n",
    "C_sig = 0.02\n",
    "P_sig = 0.06\n",
    "I_sig = 0.01\n",
    "\n",
    "n_count_tri = np.zeros((3,3,3))\n",
    "p_ave_tri = np.zeros((3,3,3,3))\n",
    "\n",
    "def cond_test(val,base,sig):\n",
    "    \n",
    "    conds = [val < base-sig,val > base-sig and val < base+sig,val > base+sig]\n",
    "    \n",
    "    return conds.index(True)-1\n",
    "        \n",
    "T_ind = []\n",
    "C_ind = []\n",
    "P_ind = []\n",
    "I_ind = []\n",
    "for ind in range(0,n_data):\n",
    "    T_ind.append(cond_test(temp[ind],T_base,T_sig))\n",
    "    C_ind.append(cond_test(cloud[ind],C_base,C_sig))\n",
    "    P_ind.append(cond_test(precip[ind],P_base,P_sig))\n",
    "    I_ind.append(cond_test(icream[ind],I_base,I_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tot_data = np.asarray([T_ind,C_ind,P_ind,I_ind]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(tot_data[0:5,:])\n",
    "print(type(tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
