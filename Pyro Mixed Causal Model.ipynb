{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.integrate import odeint\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pybel as pb\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Class of Causal Graph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create generic discrete probability function\n",
    "class cg_node():\n",
    "    def __init__(self,n_inputs,name,node_type):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.name = name\n",
    "        self.node_type = node_type\n",
    "        \n",
    "        if n_inputs == 0:\n",
    "            self.label = 'exogenous'\n",
    "        else:\n",
    "            self.label = 'endogenous'\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def batch_calc(self):\n",
    "        # break up data into mini-batches for training the node parameters\n",
    "        self.bat_per_epoch = int(self.n_data/self.batch_size)\n",
    "\n",
    "        indices = np.zeros((self.batch_size,self.bat_per_epoch)).astype(int)\n",
    "        ind_temp = np.linspace(0,self.n_data-1,self.n_data).astype(int)\n",
    "        np.random.shuffle(ind_temp)\n",
    "\n",
    "        for i in range(0,self.bat_per_epoch):\n",
    "            indices[:,i] = ind_temp[(self.batch_size*i):(self.batch_size*(i+1))]\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def reg_calc_bin(self,data_vec,var_j,var_jk):\n",
    "        # calculate the probability associated with the Bernoulli distribution given the input data\n",
    "        # use sigmoid to ensure that 0 < p < 1\n",
    "        return torch.sigmoid(torch.matmul(data_vec,var_j[:self.n_inputs]) + var_j[self.n_inputs] \n",
    "            + torch.sum(torch.matmul(data_vec,var_jk)*data_vec,dim=-1))\n",
    "\n",
    "    def reg_calc_gamma(self,data_vec,var_j,var_jk):\n",
    "        # calculate the alpha or beta for the Gamma distribution given the input data\n",
    "        # use abs to ensure that alpha and beta are >= 0\n",
    "        return torch.abs(torch.matmul(data_vec,var_j[:self.n_inputs]) + var_j[self.n_inputs] \n",
    "            + torch.sum(torch.matmul(data_vec,var_jk)*data_vec,dim=-1))\n",
    "\n",
    "    def bin_log_fcn(self,data_in,p_var):\n",
    "        # calculate negative (for minimization, not maximization) log-likelihood for the binary variable case\n",
    "        return -torch.mean(data_in*torch.log(p_var) + (1-data_in)*torch.log(1-p_var))\n",
    "\n",
    "    def gamma_log_fcn(self,data_in,alpha_var,beta_var):\n",
    "        # calculate negative (for minimization, not maximization) log-likelihood for the continuous variable case\n",
    "        return -torch.mean(alpha_var*torch.log(beta_var) + (alpha_var-1)*torch.log(data_in)\n",
    "            - beta_var*data_in - torch.lgamma(alpha_var))\n",
    "    \n",
    "    def p_init(self,input_data,var_data):\n",
    "        # calculate probability distribution parameters for node probability distributions\n",
    "        # distribution parameters are quadratic functions of input variables\n",
    "        # assumes that output variables are either Benoulli- or Gamma-distributed\n",
    "        \n",
    "        # need to adjust max_epochs and learning rate\n",
    "        \n",
    "        self.n_data = len(input_data)\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.var_data = var_data\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.max_epoch = 100*self.n_inputs\n",
    "        self.num_iters = int(self.n_data/self.batch_size)\n",
    "        \n",
    "        if self.n_inputs > 0:\n",
    "            ind_key = np.zeros((self.batch_size,self.num_iters,self.max_epoch))\n",
    "            ind_key = ind_key.astype(int)\n",
    "            for i in range(0,self.max_epoch):\n",
    "                ind_key[:,:,i] = self.batch_calc()\n",
    "                \n",
    "            self.ind_key = ind_key\n",
    "\n",
    "        if self.node_type == 'binary':\n",
    "            \n",
    "            if self.n_inputs == 0:\n",
    "                self.p_jk = []\n",
    "                self.p_j = -torch.log(1/torch.mean(var_data)-1)\n",
    "                loss_tot = bin_log_fcn(data_in,self.p_j)\n",
    "                \n",
    "            else:\n",
    "                self.p_j = torch.ones(self.n_inputs+1,requires_grad=True)\n",
    "                self.p_jk = torch.ones(self.n_inputs,self.n_inputs,requires_grad=True)\n",
    "\n",
    "                optimizer = torch.optim.Adam([self.p_j,self.p_jk], lr=1/10**(2+self.n_inputs))\n",
    "\n",
    "                # train log_like_fcn wrt self.p_j                \n",
    "                \n",
    "                for i in range(0,self.max_epoch):\n",
    "                    for j in range(0,self.bat_per_epoch):\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = self.bin_log_fcn(var_data[ind_key[:,j,i]],\n",
    "                            self.reg_calc_bin(input_data[ind_key[:,j,i],:],self.p_j,self.p_jk))\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                loss_tot = self.bin_log_fcn(var_data,self.reg_calc_bin(input_data,self.p_j,self.p_jk))                \n",
    "            \n",
    "        elif self.node_type == 'continuous':            \n",
    "            \n",
    "            if self.n_inputs == 0:\n",
    "                \n",
    "                self.alpha_jk = []\n",
    "                self.beta_jk = []\n",
    "                \n",
    "                s_temp = torch.log(torch.mean(var_data)) - torch.mean(torch.log(var_data))\n",
    "                alpha_temp = (3-s_temp + torch.sqrt((s_temp-3)**2 + 24*s_temp))/(12*s_temp)\n",
    "                beta_temp = alpha_temp/torch.mean(var_data)\n",
    "                self.alpha_j = alpha_temp\n",
    "                self.beta_j = beta_temp\n",
    "                \n",
    "                loss_tot = self.gamma_log_fcn(var_data,self.alpha_j,self.beta_j)\n",
    "                \n",
    "                # plot results\n",
    "                var_data_plot = [item.item() for item in var_data]\n",
    "                x_max = torch.max(var_data).item()\n",
    "                \n",
    "                x_pts = np.linspace(0,x_max,100)\n",
    "                alpha_val = alpha_temp.item()\n",
    "                beta_val = beta_temp.item()\n",
    "                rat_val = beta_val**alpha_val/sp.special.gamma(alpha_val)\n",
    "                y_pts = np.asarray([rat_val*item**(alpha_val-1)*np.exp(-beta_val*item) for item in x_pts])\n",
    "                \n",
    "                plt.figure()\n",
    "                hist_counts = plt.hist(var_data_plot,100)\n",
    "                plt.plot(x_pts,y_pts*np.max(hist_counts[0])/np.max(y_pts))\n",
    "                plt.show()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # initialize variables\n",
    "                \n",
    "                alpha_j_temp = np.zeros(self.n_inputs+1)\n",
    "                alpha_jk_temp = np.zeros((self.n_inputs,self.n_inputs))\n",
    "                \n",
    "                for i in range(0,self.n_inputs):\n",
    "                    alpha_j_temp[i] = 1/torch.mean(input_data[:,i]).item()\n",
    "                    for j in range(0,self.n_inputs):\n",
    "                        alpha_jk_temp[i,j] = 1/(torch.mean(input_data[:,i]).item()\n",
    "                            *torch.mean(input_data[:,i]).item())\n",
    "                var_mean = torch.mean(var_data).item()\n",
    "                        \n",
    "                alpha_j = torch.tensor(alpha_j_temp*var_mean,requires_grad=True)\n",
    "                alpha_jk = torch.tensor(alpha_jk_temp*var_mean,requires_grad=True)\n",
    "                beta_j = torch.tensor(alpha_j_temp,requires_grad=True)\n",
    "                beta_jk = torch.tensor(alpha_jk_temp,requires_grad=True)\n",
    "\n",
    "                optimizer = torch.optim.Adam([alpha_j,alpha_jk,beta_j,beta_jk], lr=1/10**(2+self.n_inputs))\n",
    "                \n",
    "                # train log_like_fcn wrt self.alpha_j, self.beta_j\n",
    "                \n",
    "                for i in range(0,self.max_epoch):\n",
    "                    for j in range(0,self.bat_per_epoch):\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = self.gamma_log_fcn(var_data[ind_key[:,j,i]],\n",
    "                            self.reg_calc_gamma(input_data[ind_key[:,j,i],:],alpha_j,alpha_jk),\n",
    "                            self.reg_calc_gamma(input_data[ind_key[:,j,i],:],beta_j,beta_jk))\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                loss_tot = self.gamma_log_fcn(var_data,self.reg_calc_gamma(input_data,alpha_j,alpha_jk),\n",
    "                    self.reg_calc_gamma(input_data,beta_j,beta_jk))\n",
    "                        \n",
    "                self.alpha_j = alpha_j\n",
    "                self.alpha_jk = alpha_jk\n",
    "                self.beta_j = beta_j\n",
    "                self.beta_jk = beta_jk\n",
    "                \n",
    "        else:\n",
    "            print('node type not supported')\n",
    "            \n",
    "        print(loss_tot)\n",
    "        print()\n",
    "        self.log_error = loss_tot\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sample(self,data_in=[]):\n",
    "        # sample your output variable given input data (for a non-exogenous variable)\n",
    "        \n",
    "        if self.node_type == 'binary':\n",
    "            if self.n_inputs == 0:\n",
    "                p_temp = torch.sigmoid(self.p_j)\n",
    "            else:\n",
    "                p_temp = self.reg_calc_bin(data_in,self.p_j,self.p_jk)\n",
    "            \n",
    "            return torch.squeeze(pyro.sample(self.name,pyro.distributions.Bernoulli(probs=p_temp)).int())\n",
    "        \n",
    "        elif self.node_type == 'continuous':\n",
    "            if self.n_inputs == 0:\n",
    "                alpha_temp = self.alpha_j\n",
    "                beta_temp = self.beta_j\n",
    "            else:                \n",
    "                alpha_temp = self.reg_calc_gamma(data_in,self.alpha_j,self.alpha_jk)\n",
    "                beta_temp = self.reg_calc_gamma(data_in,self.beta_j,self.beta_jk)\n",
    "            \n",
    "            return torch.squeeze(pyro.sample(self.name,pyro.distributions.Gamma(alpha_temp,beta_temp)))\n",
    "            \n",
    "        else:\n",
    "            print('node type not supported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Class of Causal Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cg_graph():\n",
    "    \n",
    "    def __init__(self,str_list=[],bel_graph=[],json_file=[],type_dict=[],only_creases=True):\n",
    "        \n",
    "        self.only_creases = only_creases\n",
    "        \n",
    "        edge_list = []\n",
    "\n",
    "        entity_list = []\n",
    "        \n",
    "        if str_list:\n",
    "            # construct graph from list of BEL statement strings\n",
    "\n",
    "            for item in str_list:\n",
    "\n",
    "                sub_ind = item.find('=')\n",
    "\n",
    "                sub_temp = item[:sub_ind-1]\n",
    "                obj_temp = item[sub_ind+3:]\n",
    "                \n",
    "                rel_temp = item[sub_ind:sub_ind+2]\n",
    "\n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                    \n",
    "                if only_creases:\n",
    "                    # ignore hasVariant, partOf relations\n",
    "\n",
    "                    if rel_temp.find('crease') > 0:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "                else:\n",
    "                    # check for duplicate edges\n",
    "                    nodes_temp = [sub_temp,obj_temp]\n",
    "                    list_temp = [[item[0],item[1]] for item in edge_list]\n",
    "                    if nodes_temp in list_temp:\n",
    "                        ind_temp = list_temp.index(nodes_temp)\n",
    "                        edge_list[ind_temp][2] += ',' + rel_temp\n",
    "                    else:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "        elif bel_graph:\n",
    "            # construct graph from pyBEL graph\n",
    "            \n",
    "            for item in bel_graph.edges:\n",
    "                edge_temp = bel_graph.get_edge_data(item[0],item[1],item[2])\n",
    "                sub_temp = str(item[0]).replace('\"','')\n",
    "                obj_temp = str(item[1]).replace('\"','')\n",
    "                rel_temp = edge_temp['relation']\n",
    "                \n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                if only_creases:\n",
    "                    # ignore hasVariant, partOf relations\n",
    "\n",
    "                    if rel_temp.find('crease') > 0:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "                else:\n",
    "                    # check for duplicate edges\n",
    "                    nodes_temp = [sub_temp,obj_temp]\n",
    "                    list_temp = [[item[0],item[1]] for item in edge_list]\n",
    "                    if nodes_temp in list_temp:\n",
    "                        ind_temp = list_temp.index(nodes_temp)\n",
    "                        edge_list[ind_temp][2] += ',' + rel_temp\n",
    "                    else:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "        elif json_file:\n",
    "            # construct graph from json file\n",
    "            \n",
    "            file1 = open(json_file)\n",
    "            j_str = file1.readline()\n",
    "            file1.close()\n",
    "            loaded_json = json.loads(j_str)\n",
    "            \n",
    "            entity_list = []\n",
    "            for item in loaded_json['nodes']:\n",
    "                entity_list.append(item['name'])\n",
    "                \n",
    "            edge_list = []\n",
    "            for item in loaded_json['edges']:\n",
    "                edge_list.append([item['from'],item['to'],''])\n",
    "        \n",
    "        n_nodes = len(entity_list)\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        adj_mat = np.zeros((n_nodes,n_nodes),dtype=int)\n",
    "\n",
    "        for item in edge_list:\n",
    "            out_ind = entity_list.index(item[0])\n",
    "            in_ind = entity_list.index(item[1])\n",
    "            adj_mat[out_ind,in_ind] = 1\n",
    "            \n",
    "        self.edge_list = edge_list\n",
    "        self.entity_list = entity_list\n",
    "        self.adj_mat = adj_mat\n",
    "        \n",
    "        self.graph = nx.DiGraph(adj_mat)\n",
    "        \n",
    "        # check to make sure that it's a DAG\n",
    "        if nx.algorithms.dag.is_directed_acyclic_graph(self.graph):\n",
    "            print('The causal graph is a acyclic')\n",
    "            \n",
    "        else:\n",
    "            print('The causal graph has cycles -- this is a problem')\n",
    "            \n",
    "            # identify edges that, if removed, would lead to the causal graph being acyclic\n",
    "            c_bas = list(nx.simple_cycles(self.graph))\n",
    "            print('There are ' + str(len(c_bas)) + ' simple cycles')\n",
    "            \n",
    "            cycle_edge_list = []\n",
    "            \n",
    "            for item in c_bas:\n",
    "                for i in range(0,len(item)):\n",
    "                    sub_temp = self.entity_list[item[i-1]]\n",
    "                    obj_temp = self.entity_list[item[i]]\n",
    "                    rel_temp = [item2[2] for item2 in edge_list if (item2[0] == sub_temp and item2[1] == obj_temp)]\n",
    "                    cycle_edge_list += [[sub_temp,obj_temp,item2] for item2 in rel_temp]\n",
    "            print('Cycle edges:')\n",
    "            for item in cycle_edge_list:\n",
    "                print(item)\n",
    "                \n",
    "        \n",
    "        node_dict = {}\n",
    "        \n",
    "        cont_list = ['a','abundance','complex','complexAbundance','geneAbundance','g','microRNAAbundance','m',\n",
    "            'populationAbundance','pop','proteinAbundance','p','reaction','rxn','rnaAbundance','r']\n",
    "        bin_list = ['activity','act','biologicalProcess','bp','pathology','path','molecularActivity','ma']\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            \n",
    "            if str_list or json_file:\n",
    "                node_type = type_dict[entity_list[i]]\n",
    "            elif bel_graph:\n",
    "                ind_temp = entity_list[i].find('(')\n",
    "                str_temp = entity_list[i][:ind_temp]\n",
    "                \n",
    "                \n",
    "                if str_temp in cont_list:\n",
    "                    node_type = 'continuous'\n",
    "                elif str_temp in bin_list:\n",
    "                    node_type = 'binary'\n",
    "                else:\n",
    "                    node_type = 'continuous'\n",
    "                    print('BEL node type ' + str_temp + ' not known -- defaulting to continuous')\n",
    "            \n",
    "            node_dict[entity_list[i]] = cg_node(np.sum(adj_mat[:,i]),entity_list[i],node_type)\n",
    "        \n",
    "        self.node_dict = node_dict\n",
    "        \n",
    "        self.cond_list = []\n",
    "        \n",
    "        self.sample_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = []\n",
    "        self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(self.adj_mat[:,i] > 0)[0] for i in range(0,n_nodes)]\n",
    "        self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,n_nodes)]\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            self.parent_name_dict[entity_list[i]] = [entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            self.child_name_dict[entity_list[i]] = [entity_list[item] for item in self.child_ind_list[i]]\n",
    "\n",
    "        return\n",
    "    \n",
    "    def remove_edge(self,edge_rem):\n",
    "        # remove all of the edges in edge_rem\n",
    "        \n",
    "        for item in edge_rem:\n",
    "            ind_remove = [i for i in range(0,len(self.edge_list)) \n",
    "                if (self.edge_list[i][0] == edge_rem[0] and self.edge_list[i][1] == edge_rem[1])]\n",
    "            for ind in ind_remove:\n",
    "                self.edge_list.remove(self.edge_list[i])\n",
    "            self.adj_mat[self.entity_list.index(item[0]),self.entity_list.index(item[1])] = 0\n",
    "            \n",
    "        self.graph = nx.DiGraph(self.adj_mat)\n",
    "        \n",
    "        self.parent_ind_list = []\n",
    "        self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(self.adj_mat[:,i] > 0)[0] for i in range(0,self.n_nodes)]\n",
    "        self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,self.n_nodes)]\n",
    "        \n",
    "        for i in range(0,self.n_nodes):\n",
    "            self.parent_name_dict[self.entity_list[i]] = [\n",
    "                self.entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            self.child_name_dict[self.entity_list[i]] = [\n",
    "                self.entity_list[item] for item in self.child_ind_list[i]]\n",
    "        return\n",
    "    \n",
    "    def prob_init(self,data_in):\n",
    "        # initialize all of the nodes' probability distributions given data_in\n",
    "        \n",
    "        exog_list = []\n",
    "        prob_dict = {}\n",
    "        \n",
    "        for name in self.node_dict:\n",
    "            i = self.entity_list.index(name)\n",
    "            data_in_temp = data_in[:,self.parent_ind_list[i]]\n",
    "            data_out_temp = data_in[:,i]\n",
    "            print(name)\n",
    "            self.node_dict[name].p_init(data_in_temp,data_out_temp)\n",
    "            \n",
    "            if self.node_dict[name].n_inputs == 0:\n",
    "                exog_list.append(name)\n",
    "            #prob_dict[name] = self.node_dict[name].prob_dist\n",
    "        \n",
    "        self.exog_list = exog_list\n",
    "        #self.prob_dict = prob_dict\n",
    "\n",
    "        return\n",
    "        \n",
    "    def model_sample(self):\n",
    "        # produce a dictionary of samples for all variables in the graph\n",
    "        \n",
    "        # define exogenous samples\n",
    "        \n",
    "        sample_dict = {}\n",
    "        \n",
    "        for item in self.exog_list:\n",
    "            sample_dict[item] = self.node_dict[item].sample()\n",
    "            \n",
    "        flag = 0\n",
    "        while flag == 0:\n",
    "            \n",
    "            # find all nodes not in sample_dict with parents entirely in sample dict and sample those nodes\n",
    "            for item in self.entity_list:\n",
    "                if (item not in sample_dict \n",
    "                    and np.all([item2 in sample_dict for item2 in self.parent_name_dict[item]])):\n",
    "                    \n",
    "                    sample_dict[item] = self.node_dict[item].sample(\n",
    "                        torch.tensor([sample_dict[item2] for item2 in self.parent_name_dict[item]]))\n",
    "            \n",
    "            # if sample dict has all of the nodes in entity list, stop\n",
    "            if sorted([item for item in sample_dict]) == sorted(self.entity_list):\n",
    "                flag = 1\n",
    "            \n",
    "        \n",
    "        return sample_dict\n",
    "    \n",
    "    def model_cond_sample(self,data_dict):\n",
    "        # sample the graph given the conditioned variables in data_dict\n",
    "        \n",
    "        data_in = {}\n",
    "        for item in data_dict:\n",
    "            data_in[item] = data_dict[item]\n",
    "        \n",
    "        cond_model = pyro.condition(self.model_sample,data=data_in)\n",
    "        return cond_model()\n",
    "        \n",
    "    def model_do_sample(self,do_dict):\n",
    "        # sample the graph given the do-variables in do_dict\n",
    "        \n",
    "        data_in = {}\n",
    "        for item in do_dict:\n",
    "            data_in[item] = do_dict[item]\n",
    "        \n",
    "        do_model = pyro.do(self.model_sample,data=data_in)\n",
    "        return do_model()\n",
    "    \n",
    "    def model_do_cond_sample(self,do_dict,data_dict):\n",
    "        # sample the graph given do-variables in do_dict and conditioned variables in data_dict\n",
    "        \n",
    "        if np.any([[item1 == item2 for item1 in do_dict] for item2 in data_dict]):\n",
    "            print('overlapping lists!')\n",
    "            return\n",
    "        else:\n",
    "            do_dict_in = {}\n",
    "            for item in do_dict:\n",
    "                do_dict_in[item] = do_dict[item]\n",
    "                \n",
    "            data_dict_in = {}\n",
    "            for item in data_dict:\n",
    "                data_dict_in[item] = data_dict[item]\n",
    "            \n",
    "            do_model = pyro.do(self.model_sample,data=do_dict_in)\n",
    "            cond_model = pyro.condition(do_model,data=data_dict_in)\n",
    "            return cond_model()\n",
    "    \n",
    "    def model_counterfact(self,obs_dict,do_dict_counter):\n",
    "        # find conditional distribution on exogenous variables given observations in obs_dict \n",
    "        # and do variable values in do_dict_counter\n",
    "        cond_dict = self.model_cond_sample(obs_dict)\n",
    "        cond_dict_temp = {}\n",
    "        for item in self.exog_list:\n",
    "            cond_dict_temp[item] = cond_dict[item]\n",
    "        \n",
    "        # evaluate observed variables given this condition distribution and do_dict_counter do-variables\n",
    "        return self.model_do_cond_sample(do_dict_counter,cond_dict_temp)\n",
    "        \n",
    "        \n",
    "    def cond_mut_info(self,target,test,cond,data_in):\n",
    "        # calculate the conditional mutual information between target and test given data_in - I(target:test|cond)\n",
    "        # just uses input data, but have to bin data (creating discrete distribution) to perform calculations\n",
    "        \n",
    "        n_data = len(data_in)\n",
    "        \n",
    "        data_in_np = np.asarray([[item2.item() for item2 in item] for item in data_in])        \n",
    "        cond_temp = cond\n",
    "        \n",
    "        if not cond:\n",
    "            # find parents of target\n",
    "            for item in target:\n",
    "                for item2 in self.parent_name_dict[item]:\n",
    "                    if item2 not in cond_temp:\n",
    "                        cond_temp.append(item2)\n",
    "        \n",
    "        \n",
    "        target_inds = [self.entity_list.index(item) for item in target]\n",
    "        test_inds = [self.entity_list.index(item) for item in test]\n",
    "        cond_inds = [self.entity_list.index(item) for item in cond_temp]\n",
    "\n",
    "        total_inds = target_inds + test_inds + cond_inds\n",
    "        n_tot = len(total_inds)\n",
    "        n_target = len(target_inds)\n",
    "        n_test = len(test_inds)\n",
    "        n_cond = len(cond_inds)\n",
    "\n",
    "        # bin the incoming data\n",
    "        data_bin = np.histogramdd((data_in_np[:,total_inds]),bins=10)[0]/n_data\n",
    "\n",
    "        \n",
    "        # calculate each joint entropy\n",
    "        \n",
    "        all_inds = list(range(0,n_tot))\n",
    "        \n",
    "        p_z = np.sum(data_bin,tuple(all_inds[:n_target+n_test]))\n",
    "        H_z = -np.sum(p_z*np.log(p_z+1e-6))\n",
    "        \n",
    "        p_xz = np.sum(data_bin,tuple(all_inds[n_target:n_target+n_test]))\n",
    "        H_xz = -np.sum(p_xz*np.log(p_xz+1e-6))\n",
    "        \n",
    "        p_yz = np.sum(data_bin,tuple(all_inds[:n_target]))\n",
    "        H_yz = -np.sum(p_yz*np.log(p_yz+1e-6))\n",
    "        \n",
    "        H_xyz = -np.sum(data_bin*np.log(data_bin+1e-6))\n",
    "                \n",
    "        return H_xz + H_yz - H_xyz - H_z\n",
    "        \n",
    "    def g_test(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest \n",
    "        # determine if causal graph captures underlying distribution\n",
    "        # have to bin data to perform calculations\n",
    "        \n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        \n",
    "        if self.node_dict[name[0]].node_type == 'binary':\n",
    "            # bin the data\n",
    "            binned_data = torch.histc(data_in[:,name_ind],2,-0.5,1.5)\n",
    "            \n",
    "            # generate sample data\n",
    "            data_samp = torch.tensor([self.model_sample()[name[0]] for i in range(0,len(data_in))])\n",
    "            binned_samp = torch.histc(data_samp,2,-0.5,1.5)\n",
    "            \n",
    "        else:\n",
    "            data_max = torch.max(data_in[:,name_ind])\n",
    "            data_min = torch.min(data_in[:,name_ind])\n",
    "\n",
    "            # bin the data\n",
    "            binned_data = torch.histc(data_in[:,name_ind],100,data_min,data_max)\n",
    "\n",
    "            # generate sample data\n",
    "            data_samp = torch.tensor([self.model_sample()[name[0]] for i in range(0,len(data_in))])\n",
    "            binned_samp = torch.histc(data_samp,100,data_min,data_max)\n",
    "        \n",
    "        \n",
    "        g_val = 2*torch.sum(binned_data*torch.log(binned_data/(binned_samp+1e-6)))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "        \n",
    "  \n",
    "    def write_to_cf(self,filename,spacing):\n",
    "        # write the causal graph to a text file to import into causal fusion\n",
    "        \n",
    "        pos_dict = nx.drawing.layout.planar_layout(self.graph)\n",
    "        \n",
    "        write_dict = {}\n",
    "        write_dict['name'] = 'causal_graph'\n",
    "        \n",
    "        # write nodes\n",
    "        write_dict['nodes'] = []\n",
    "        for i in range(0,len(self.entity_list)):\n",
    "            name = self.entity_list[i]\n",
    "            \n",
    "            write_dict['nodes'].append({})\n",
    "            \n",
    "            write_dict['nodes'][-1]['id'] = 'node' + str(i)\n",
    "            write_dict['nodes'][-1]['name'] = name\n",
    "            write_dict['nodes'][-1]['label'] = name\n",
    "            write_dict['nodes'][-1]['type'] = 'basic'\n",
    "            write_dict['nodes'][-1]['metadata'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['x'] = spacing*pos_dict[i][0]\n",
    "            write_dict['nodes'][-1]['metadata']['y'] = spacing*pos_dict[i][1]\n",
    "            write_dict['nodes'][-1]['metadata']['label'] = ''\n",
    "            write_dict['nodes'][-1]['metadata']['shape'] = 'ellipse'\n",
    "            write_dict['nodes'][-1]['metadata']['fontSize'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['sizeLabelMode'] = 5\n",
    "            write_dict['nodes'][-1]['metadata']['font'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['font']['size'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['size'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeId'] = 'node' + str(i) + 'ID'\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset']['y'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset']['y'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['color'] = '#00000080'\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['size'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['y'] = 0\n",
    "            \n",
    "        # write edges\n",
    "        write_dict['edges'] = []\n",
    "        \n",
    "        for i in range(0,len(self.edge_list)):\n",
    "            \n",
    "            item = self.edge_list[i]\n",
    "            from_node = self.entity_list.index(item[0])\n",
    "            to_node = self.entity_list.index(item[1])\n",
    "            \n",
    "            write_dict['edges'].append({})\n",
    "            \n",
    "            write_dict['edges'][-1]['id'] = 'node' + str(from_node) + '->node' + str(to_node)\n",
    "            write_dict['edges'][-1]['from'] = item[0]\n",
    "            write_dict['edges'][-1]['to'] = item[1]\n",
    "            write_dict['edges'][-1]['type'] = 'directed'\n",
    "            write_dict['edges'][-1]['metadata'] = {}\n",
    "            write_dict['edges'][-1]['metadata']['isLabelDraggable'] = True\n",
    "            write_dict['edges'][-1]['metadata']['label'] = ''\n",
    "            \n",
    "        \n",
    "        write_dict['task'] = {}\n",
    "        \n",
    "        write_dict['metadata'] = {}\n",
    "        \n",
    "        write_dict['project_id'] = '123456789'\n",
    "        write_dict['_fileType'] = 'graph'\n",
    "                \n",
    "        with open(filename + '.json', 'w') as json_file:\n",
    "            json.dump(write_dict, json_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indep_vars(n_samples):\n",
    "    \n",
    "    T_list = []\n",
    "    C_list = []\n",
    "    P_list = []\n",
    "    \n",
    "    for i in range(0,n_samples):\n",
    "        \n",
    "        #x = pyro.sample(\"x_{}\".format(i), pyro.distributions.Normal(20,5))\n",
    "        \n",
    "        #T_temp = pyro.distributions.Normal(20,5).sample()\n",
    "        #C_temp = 0.5*pyro.distributions.Beta(1,1+T_temp/10).sample() + 0.5*pyro.distributions.Uniform(0,1).sample()\n",
    "        #P_temp = (0.5*pyro.distributions.Exponential(1).sample() \n",
    "            #+ 0.5*pyro.distributions.Exponential(1/(C_temp+1)).sample())\n",
    "        \n",
    "        T_list.append(pyro.sample(\"T_{}\".format(i), pyro.distributions.LogNormal(2.96,0.2)))\n",
    "        \n",
    "        C_list.append(0.5*pyro.sample(\"C1_{}\".format(i),pyro.distributions.Beta(1,1+T_list[-1]/10)) \n",
    "            + 0.5*pyro.sample(\"C2_{}\".format(i),pyro.distributions.Uniform(0,1)))\n",
    "        P_list.append(0.5*pyro.sample(\"P1_{}\".format(i), pyro.distributions.Exponential(1))\n",
    "            + 0.5*pyro.sample(\"P2.{}\".format(i),pyro.distributions.Exponential(1/(C_list[-1]+1))))\n",
    "        \n",
    "    return T_list,C_list,P_list\n",
    "\n",
    "def dep_vars(T_list,C_list,P_list):\n",
    "    \n",
    "    n_pts = len(T_list)\n",
    "    \n",
    "    I_list = []\n",
    "    \n",
    "    for i in range(0,n_pts):\n",
    "        \n",
    "        T_temp = T_list[i]\n",
    "        C_temp = C_list[i]\n",
    "        P_temp = P_list[i]\n",
    "        \n",
    "        if P_temp > 2.5 or T_temp < 15:\n",
    "            I_list.append(pyro.sample(\"I_{}\".format(i),pyro.distributions.Bernoulli(0))+1e-6)\n",
    "        else:\n",
    "            I_list.append(pyro.sample(\"I_{}\".format(i),\n",
    "                pyro.distributions.Beta(2*(2.5-P_temp)*(T_temp-12)/(2.5*12),2))+1e-6)\n",
    "            #I_temp = torch.tensor(0.5)\n",
    "        \n",
    "    return I_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "temp,cloud,precip = indep_vars(n_data)\n",
    "icream = dep_vars(temp,cloud,precip)\n",
    "tot_data = torch.Tensor([temp,cloud,precip,icream])\n",
    "tot_data = tot_data.T\n",
    "print(tot_data.size())\n",
    "print(tot_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Graph from BEL Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = ['temp =| cloudy','cloudy => rainy','temp => icream','rainy =| icream']\n",
    "type_dict = {}\n",
    "type_dict['temp'] = 'continuous'\n",
    "type_dict['cloudy'] = 'continuous'\n",
    "type_dict['rainy'] = 'continuous'\n",
    "type_dict['icream'] = 'continuous'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_test = cg_graph(str_list=str_list,type_dict=type_dict,only_creases=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Graph from json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_test = cg_graph(json_file='icream.txt',type_dict=type_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Graph from PyBEL graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bel_temp = pb.from_bel_script('sag_bel_graph.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_test = cg_graph(bel_graph=bel_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Various Graph Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file that Can be Read by Causal Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test.write_to_cf('test',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Graph Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_test.prob_init(tot_data)\n",
    "print(graph_test.exog_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Graph Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(item)\n",
    "    print(graph_test.node_dict[item].name)\n",
    "    print(graph_test.node_dict[item].node_type)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(graph_test.node_dict[item].alpha_j)\n",
    "    print(graph_test.node_dict[item].alpha_jk)\n",
    "    print(graph_test.node_dict[item].beta_j)\n",
    "    print(graph_test.node_dict[item].beta_jk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sample the Graph under Various Situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_test.model_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cond_dict = {}\n",
    "cond_dict['cloudy'] = torch.Tensor([0.5])\n",
    "\n",
    "cond_test = graph_test.model_cond_sample(cond_dict)\n",
    "\n",
    "for item in cond_test:\n",
    "    print(item)\n",
    "    print(cond_test[item])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_dict = {}\n",
    "do_dict['rainy'] = torch.Tensor([2.5])\n",
    "\n",
    "do_test = graph_test.model_do_sample(do_dict)\n",
    "\n",
    "for item in do_test:\n",
    "    print(item)\n",
    "    print(do_test[item])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cond_test = graph_test.model_do_cond_sample(do_dict,cond_dict)\n",
    "for item in do_cond_test:\n",
    "    print(item)\n",
    "    print(do_cond_test[item])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = {}\n",
    "obs_dict['icream'] = torch.Tensor([0.5])\n",
    "obs_dict['rainy'] = torch.Tensor([0.8])\n",
    "\n",
    "do_dict = {}\n",
    "do_dict['rainy'] = torch.Tensor([1.5])\n",
    "\n",
    "\n",
    "counter_test = graph_test.model_counterfact(obs_dict,do_dict)\n",
    "for item in counter_test:\n",
    "    print(item)\n",
    "    print(counter_test[item])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Statistical Tests on Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(graph_test.cond_mut_info(['icream'],['rainy'],['temp'],tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graph_test.g_test(['icream'],tot_data)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
