{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.integrate import odeint\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pybel as pb\n",
    "\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Class of Causal Graph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create generic discrete probability function\n",
    "class cg_node():\n",
    "    def __init__(self,n_inputs,name,node_type):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.name = name\n",
    "        self.node_type = node_type\n",
    "        \n",
    "        if n_inputs == 0:\n",
    "            self.label = 'exogenous'\n",
    "        else:\n",
    "            self.label = 'endogenous'\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def batch_calc(self):\n",
    "        # break up data into mini-batches for training the node parameters\n",
    "        self.bat_per_epoch = int(self.n_data/self.batch_size)\n",
    "\n",
    "        indices = np.zeros((self.batch_size,self.bat_per_epoch)).astype(int)\n",
    "        ind_temp = np.linspace(0,self.n_data-1,self.n_data).astype(int)\n",
    "        np.random.shuffle(ind_temp)\n",
    "\n",
    "        for i in range(0,self.bat_per_epoch):\n",
    "            indices[:,i] = ind_temp[(self.batch_size*i):(self.batch_size*(i+1))]\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def reg_calc_bin(self,data_vec,var_j,var_jk):\n",
    "        # calculate the probability associated with the Bernoulli distribution given the input data\n",
    "        # use sigmoid to ensure that 0 < p < 1\n",
    "        return torch.sigmoid(torch.matmul(data_vec,var_j[:self.n_inputs]) + var_j[self.n_inputs] \n",
    "            + torch.sum(torch.matmul(data_vec,var_jk)*data_vec,dim=-1))\n",
    "\n",
    "    def reg_calc_gamma(self,data_vec,var_j,var_jk):\n",
    "        # calculate the alpha or beta for the Gamma distribution given the input data\n",
    "        # use abs to ensure that alpha and beta are >= 0\n",
    "        return torch.abs(torch.matmul(data_vec,var_j[:self.n_inputs]) + var_j[self.n_inputs] \n",
    "            + torch.sum(torch.matmul(data_vec,var_jk)*data_vec,dim=-1))\n",
    "\n",
    "    def bin_log_fcn(self,data_in,p_var):\n",
    "        # calculate negative (for minimization, not maximization) log-likelihood for the binary variable case\n",
    "        return -torch.mean(data_in*torch.log(p_var) + (1-data_in)*torch.log(1-p_var))\n",
    "\n",
    "    def gamma_log_fcn(self,data_in,alpha_var,beta_var):\n",
    "        # calculate negative (for minimization, not maximization) log-likelihood for the continuous variable case\n",
    "        return -torch.mean(alpha_var*torch.log(beta_var) + (alpha_var-1)*torch.log(data_in)\n",
    "            - beta_var*data_in - torch.lgamma(alpha_var))\n",
    "    \n",
    "    def p_init(self,input_data,var_data):\n",
    "        # calculate probability distribution parameters for node probability distributions\n",
    "        # distribution parameters are quadratic functions of input variables\n",
    "        # assumes that output variables are either Benoulli- or Gamma-distributed\n",
    "        \n",
    "        # need to adjust max_epochs and learning rate\n",
    "        \n",
    "        self.n_data = len(input_data)\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.var_data = var_data\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.max_epoch = 100*self.n_inputs\n",
    "        self.num_iters = int(self.n_data/self.batch_size)\n",
    "        \n",
    "        if self.n_inputs > 0:\n",
    "            ind_key = np.zeros((self.batch_size,self.num_iters,self.max_epoch))\n",
    "            ind_key = ind_key.astype(int)\n",
    "            for i in range(0,self.max_epoch):\n",
    "                ind_key[:,:,i] = self.batch_calc()\n",
    "                \n",
    "            self.ind_key = ind_key\n",
    "\n",
    "        if self.node_type == 'binary':\n",
    "            \n",
    "            if self.n_inputs == 0:\n",
    "                self.p_jk = []\n",
    "                self.p_j = -torch.log(1/torch.mean(var_data)-1)\n",
    "                loss_tot = bin_log_fcn(data_in,self.p_j)\n",
    "                \n",
    "            else:\n",
    "                self.p_j = torch.ones(self.n_inputs+1,requires_grad=True)\n",
    "                self.p_jk = torch.ones(self.n_inputs,self.n_inputs,requires_grad=True)\n",
    "\n",
    "                optimizer = torch.optim.Adam([self.p_j,self.p_jk], lr=1/10**(2+self.n_inputs))\n",
    "\n",
    "                # train log_like_fcn wrt self.p_j                \n",
    "                \n",
    "                for i in range(0,self.max_epoch):\n",
    "                    for j in range(0,self.bat_per_epoch):\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = self.bin_log_fcn(var_data[ind_key[:,j,i]],\n",
    "                            self.reg_calc_bin(input_data[ind_key[:,j,i],:],self.p_j,self.p_jk))\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                loss_tot = self.bin_log_fcn(var_data,self.reg_calc_bin(input_data,self.p_j,self.p_jk))                \n",
    "            \n",
    "        elif self.node_type == 'continuous':            \n",
    "            \n",
    "            if self.n_inputs == 0:\n",
    "                \n",
    "                self.alpha_jk = []\n",
    "                self.beta_jk = []\n",
    "                \n",
    "                s_temp = torch.log(torch.mean(var_data)) - torch.mean(torch.log(var_data))\n",
    "                alpha_temp = (3-s_temp + torch.sqrt((s_temp-3)**2 + 24*s_temp))/(12*s_temp)\n",
    "                beta_temp = alpha_temp/torch.mean(var_data)\n",
    "                self.alpha_j = alpha_temp\n",
    "                self.beta_j = beta_temp\n",
    "                \n",
    "                loss_tot = self.gamma_log_fcn(var_data,self.alpha_j,self.beta_j)\n",
    "                \n",
    "                # plot results\n",
    "                var_data_plot = [item.item() for item in var_data]\n",
    "                x_max = torch.max(var_data).item()\n",
    "                \n",
    "                x_pts = np.linspace(0,x_max,100)\n",
    "                alpha_val = alpha_temp.item()\n",
    "                beta_val = beta_temp.item()\n",
    "                rat_val = beta_val**alpha_val/sp.special.gamma(alpha_val)\n",
    "                y_pts = np.asarray([rat_val*item**(alpha_val-1)*np.exp(-beta_val*item) for item in x_pts])\n",
    "                \n",
    "                plt.figure()\n",
    "                hist_counts = plt.hist(var_data_plot,100)\n",
    "                plt.plot(x_pts,y_pts*np.max(hist_counts[0])/np.max(y_pts))\n",
    "                plt.show()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # initialize variables\n",
    "                \n",
    "                alpha_j_temp = np.zeros(self.n_inputs+1)\n",
    "                alpha_jk_temp = np.zeros((self.n_inputs,self.n_inputs))\n",
    "                \n",
    "                for i in range(0,self.n_inputs):\n",
    "                    alpha_j_temp[i] = 1/torch.mean(input_data[:,i]).item()\n",
    "                    for j in range(0,self.n_inputs):\n",
    "                        alpha_jk_temp[i,j] = 1/(torch.mean(input_data[:,i]).item()\n",
    "                            *torch.mean(input_data[:,i]).item())\n",
    "                var_mean = torch.mean(var_data).item()\n",
    "                        \n",
    "                alpha_j = torch.tensor(alpha_j_temp*var_mean,requires_grad=True)\n",
    "                alpha_jk = torch.tensor(alpha_jk_temp*var_mean,requires_grad=True)\n",
    "                beta_j = torch.tensor(alpha_j_temp,requires_grad=True)\n",
    "                beta_jk = torch.tensor(alpha_jk_temp,requires_grad=True)\n",
    "\n",
    "                optimizer = torch.optim.Adam([alpha_j,alpha_jk,beta_j,beta_jk], lr=1/10**(2+self.n_inputs))\n",
    "                \n",
    "                # train log_like_fcn wrt self.alpha_j, self.beta_j\n",
    "                \n",
    "                for i in range(0,self.max_epoch):\n",
    "                    for j in range(0,self.bat_per_epoch):\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = self.gamma_log_fcn(var_data[ind_key[:,j,i]],\n",
    "                            self.reg_calc_gamma(input_data[ind_key[:,j,i],:],alpha_j,alpha_jk),\n",
    "                            self.reg_calc_gamma(input_data[ind_key[:,j,i],:],beta_j,beta_jk))\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                loss_tot = self.gamma_log_fcn(var_data,self.reg_calc_gamma(input_data,alpha_j,alpha_jk),\n",
    "                    self.reg_calc_gamma(input_data,beta_j,beta_jk))\n",
    "                        \n",
    "                self.alpha_j = alpha_j\n",
    "                self.alpha_jk = alpha_jk\n",
    "                self.beta_j = beta_j\n",
    "                self.beta_jk = beta_jk\n",
    "                \n",
    "        else:\n",
    "            print('node type not supported')\n",
    "            \n",
    "        print(loss_tot)\n",
    "        print()\n",
    "        self.log_error = loss_tot\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sample(self,data_in=[]):\n",
    "        # sample your output variable given input data (for a non-exogenous variable)\n",
    "        \n",
    "        if self.node_type == 'binary':\n",
    "            if self.n_inputs == 0:\n",
    "                p_temp = torch.sigmoid(self.p_j)\n",
    "            else:\n",
    "                p_temp = self.reg_calc_bin(data_in,self.p_j,self.p_jk)\n",
    "            \n",
    "            return torch.squeeze(pyro.sample(self.name,pyro.distributions.Bernoulli(probs=p_temp)).int())\n",
    "        \n",
    "        elif self.node_type == 'continuous':\n",
    "            if self.n_inputs == 0:\n",
    "                alpha_temp = self.alpha_j\n",
    "                beta_temp = self.beta_j\n",
    "            else:                \n",
    "                alpha_temp = self.reg_calc_gamma(data_in,self.alpha_j,self.alpha_jk)\n",
    "                beta_temp = self.reg_calc_gamma(data_in,self.beta_j,self.beta_jk)\n",
    "            \n",
    "            return torch.squeeze(pyro.sample(self.name,pyro.distributions.Gamma(alpha_temp,beta_temp)))\n",
    "            \n",
    "        else:\n",
    "            print('node type not supported')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cg_graph():\n",
    "    \n",
    "    def __init__(self,str_list=[],bel_graph=[],type_dict=[],only_creases=True):\n",
    "        \n",
    "        self.only_creases = only_creases\n",
    "        \n",
    "        edge_list = []\n",
    "\n",
    "        entity_list = []\n",
    "        \n",
    "        if str_list:\n",
    "\n",
    "            for item in str_list:\n",
    "\n",
    "                sub_ind = item.find('=')\n",
    "\n",
    "                sub_temp = item[:sub_ind-1]\n",
    "                obj_temp = item[sub_ind+3:]\n",
    "                \n",
    "                rel_temp = item[sub_ind:sub_ind+2]\n",
    "\n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                    \n",
    "                if only_creases:\n",
    "                    # ignore hasVariant, partOf relations\n",
    "\n",
    "                    if rel_temp.find('crease') > 0:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "                else:\n",
    "                    # check for duplicate edges\n",
    "                    nodes_temp = [sub_temp,obj_temp]\n",
    "                    list_temp = [[item[0],item[1]] for item in edge_list]\n",
    "                    if nodes_temp in list_temp:\n",
    "                        ind_temp = list_temp.index(nodes_temp)\n",
    "                        edge_list[ind_temp][2] += ',' + rel_temp\n",
    "                    else:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "        elif bel_graph:\n",
    "            \n",
    "            for item in bel_graph.edges:\n",
    "                edge_temp = bel_graph.get_edge_data(item[0],item[1],item[2])\n",
    "                sub_temp = str(item[0]).replace('\"','')\n",
    "                obj_temp = str(item[1]).replace('\"','')\n",
    "                rel_temp = edge_temp['relation']\n",
    "                \n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                if only_creases:\n",
    "                    # ignore hasVariant, partOf relations\n",
    "\n",
    "                    if rel_temp.find('crease') > 0:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "                else:\n",
    "                    # check for duplicate edges\n",
    "                    nodes_temp = [sub_temp,obj_temp]\n",
    "                    list_temp = [[item[0],item[1]] for item in edge_list]\n",
    "                    if nodes_temp in list_temp:\n",
    "                        ind_temp = list_temp.index(nodes_temp)\n",
    "                        edge_list[ind_temp][2] += ',' + rel_temp\n",
    "                    else:\n",
    "                        edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "\n",
    "        n_nodes = len(entity_list)\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        adj_mat = np.zeros((n_nodes,n_nodes),dtype=int)\n",
    "\n",
    "        for item in edge_list:\n",
    "            out_ind = entity_list.index(item[0])\n",
    "            in_ind = entity_list.index(item[1])\n",
    "            adj_mat[out_ind,in_ind] = 1\n",
    "            \n",
    "        self.edge_list = edge_list\n",
    "        self.entity_list = entity_list\n",
    "        self.adj_mat = adj_mat\n",
    "        \n",
    "        self.graph = nx.DiGraph(adj_mat)\n",
    "        \n",
    "        # check to make sure that it's a DAG\n",
    "        if nx.algorithms.dag.is_directed_acyclic_graph(self.graph):\n",
    "            print('The causal graph is a acyclic')\n",
    "            \n",
    "        else:\n",
    "            print('The causal graph has cycles -- this is a problem')\n",
    "            \n",
    "            # identify edges that, if removed, would lead to the causal graph being acyclic\n",
    "            c_bas = list(nx.simple_cycles(self.graph))\n",
    "            print('There are ' + str(len(c_bas)) + ' simple cycles')\n",
    "            \n",
    "            cycle_edge_list = []\n",
    "            \n",
    "            for item in c_bas:\n",
    "                for i in range(0,len(item)):\n",
    "                    sub_temp = self.entity_list[item[i-1]]\n",
    "                    obj_temp = self.entity_list[item[i]]\n",
    "                    rel_temp = [item2[2] for item2 in edge_list if (item2[0] == sub_temp and item2[1] == obj_temp)]\n",
    "                    cycle_edge_list += [[sub_temp,obj_temp,item2] for item2 in rel_temp]\n",
    "            print('Cycle edges:')\n",
    "            for item in cycle_edge_list:\n",
    "                print(item)\n",
    "                \n",
    "        \n",
    "        node_dict = {}\n",
    "        \n",
    "        cont_list = ['a','abundance','complex','complexAbundance','geneAbundance','g','microRNAAbundance','m',\n",
    "            'populationAbundance','pop','proteinAbundance','p','reaction','rxn','rnaAbundance','r']\n",
    "        bin_list = ['activity','act','biologicalProcess','bp','molecularActivity','ma']\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            \n",
    "            if str_list:\n",
    "                node_type = type_dict[entity_list[i]]\n",
    "            elif bel_graph:\n",
    "                ind_temp = entity_list[i].find('(')\n",
    "                str_temp = entity_list[i][:ind_temp]\n",
    "                \n",
    "                \n",
    "                if str_temp in cont_list:\n",
    "                    node_type = 'continuous'\n",
    "                elif str_temp in bin_list:\n",
    "                    node_type = 'binary'\n",
    "                else:\n",
    "                    node_type = 'continuous'\n",
    "                    print('BEL node type ' + str_temp + ' not known -- defaulting to continuous')\n",
    "            \n",
    "            node_dict[entity_list[i]] = cg_node(np.sum(adj_mat[:,i]),entity_list[i],node_type)\n",
    "        \n",
    "        self.node_dict = node_dict\n",
    "        \n",
    "        self.cond_list = []\n",
    "        \n",
    "        self.sample_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = []\n",
    "        self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(self.adj_mat[:,i] > 0)[0] for i in range(0,n_nodes)]\n",
    "        self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,n_nodes)]\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            self.parent_name_dict[entity_list[i]] = [entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            self.child_name_dict[entity_list[i]] = [entity_list[item] for item in self.child_ind_list[i]]\n",
    "\n",
    "        return\n",
    "    \n",
    "    def remove_edge(self,edge_rem):\n",
    "        # remove all of the edges in edge_rem\n",
    "        \n",
    "        for item in edge_rem:\n",
    "            ind_remove = [i for i in range(0,len(self.edge_list)) \n",
    "                if (self.edge_list[i][0] == edge_rem[0] and self.edge_list[i][1] == edge_rem[1])]\n",
    "            for ind in ind_remove:\n",
    "                self.edge_list.remove(self.edge_list[i])\n",
    "            self.adj_mat[self.entity_list.index(item[0]),self.entity_list.index(item[1])] = 0\n",
    "            \n",
    "        self.graph = nx.DiGraph(self.adj_mat)\n",
    "        \n",
    "        self.parent_ind_list = []\n",
    "        self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(self.adj_mat[:,i] > 0)[0] for i in range(0,self.n_nodes)]\n",
    "        self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,self.n_nodes)]\n",
    "        \n",
    "        for i in range(0,self.n_nodes):\n",
    "            self.parent_name_dict[self.entity_list[i]] = [\n",
    "                self.entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            self.child_name_dict[self.entity_list[i]] = [\n",
    "                self.entity_list[item] for item in self.child_ind_list[i]]\n",
    "        return\n",
    "    \n",
    "    def prob_init(self,data_in):\n",
    "        # initialize all of the nodes' probability distributions given data_in\n",
    "        \n",
    "        exog_list = []\n",
    "        prob_dict = {}\n",
    "        \n",
    "        for name in self.node_dict:\n",
    "            i = self.entity_list.index(name)\n",
    "            data_in_temp = data_in[:,self.parent_ind_list[i]]\n",
    "            data_out_temp = data_in[:,i]\n",
    "            print(name)\n",
    "            self.node_dict[name].p_init(data_in_temp,data_out_temp)\n",
    "            \n",
    "            if self.node_dict[name].n_inputs == 0:\n",
    "                exog_list.append(name)\n",
    "            #prob_dict[name] = self.node_dict[name].prob_dist\n",
    "        \n",
    "        self.exog_list = exog_list\n",
    "        #self.prob_dict = prob_dict\n",
    "\n",
    "        return\n",
    "        \n",
    "    def model_sample(self):\n",
    "        # produce a dictionary of samples for all variables in the graph\n",
    "        \n",
    "        # define exogenous samples\n",
    "        \n",
    "        sample_dict = {}\n",
    "        \n",
    "        for item in self.exog_list:\n",
    "            sample_dict[item] = self.node_dict[item].sample()\n",
    "            \n",
    "        flag = 0\n",
    "        while flag == 0:\n",
    "            \n",
    "            # find all nodes not in sample_dict with parents entirely in sample dict and sample those nodes\n",
    "            for item in self.entity_list:\n",
    "                if (item not in sample_dict \n",
    "                    and np.all([item2 in sample_dict for item2 in self.parent_name_dict[item]])):\n",
    "                    \n",
    "                    sample_dict[item] = self.node_dict[item].sample(\n",
    "                        torch.tensor([sample_dict[item2] for item2 in self.parent_name_dict[item]]))\n",
    "            \n",
    "            # if sample dict has all of the nodes in entity list, stop\n",
    "            if sorted([item for item in sample_dict]) == sorted(self.entity_list):\n",
    "                flag = 1\n",
    "            \n",
    "        \n",
    "        return sample_dict\n",
    "    \n",
    "    def model_cond_sample(self,data_dict):\n",
    "        # sample the graph given the conditioned variables in data_dict\n",
    "        \n",
    "        data_in = {}\n",
    "        for item in data_dict:\n",
    "            data_in[item] = data_dict[item]\n",
    "        \n",
    "        cond_model = pyro.condition(self.model_sample,data=data_in)\n",
    "        return cond_model()\n",
    "        \n",
    "    def model_do_sample(self,do_dict):\n",
    "        # sample the graph given the do-variables in do_dict\n",
    "        \n",
    "        data_in = {}\n",
    "        for item in do_dict:\n",
    "            data_in[item] = do_dict[item]\n",
    "        \n",
    "        do_model = pyro.do(self.model_sample,data=data_in)\n",
    "        return do_model()\n",
    "    \n",
    "    def model_do_cond_sample(self,do_dict,data_dict):\n",
    "        # sample the graph given do-variables in do_dict and conditioned variables in data_dict\n",
    "        \n",
    "        if np.any([[item1 == item2 for item1 in do_dict] for item2 in data_dict]):\n",
    "            print('overlapping lists!')\n",
    "            return\n",
    "        else:\n",
    "            do_dict_in = {}\n",
    "            for item in do_dict:\n",
    "                do_dict_in[item] = do_dict[item]\n",
    "                \n",
    "            data_dict_in = {}\n",
    "            for item in data_dict:\n",
    "                data_dict_in[item] = data_dict[item]\n",
    "            \n",
    "            do_model = pyro.do(self.model_sample,data=do_dict_in)\n",
    "            cond_model = pyro.condition(do_model,data=data_dict_in)\n",
    "            return cond_model()\n",
    "    \n",
    "    def model_counterfact(self,obs_dict,do_dict_counter):\n",
    "        # find conditional distribution on exogenous variables given observations in obs_dict \n",
    "        # and do variable values in do_dict_counter\n",
    "        cond_dict = self.model_cond_sample(obs_dict)\n",
    "        cond_dict_temp = {}\n",
    "        for item in self.exog_list:\n",
    "            cond_dict_temp[item] = cond_dict[item]\n",
    "        \n",
    "        # evaluate observed variables given this condition distribution and do_dict_counter do-variables\n",
    "        return self.model_do_cond_sample(do_dict_counter,cond_dict_temp)\n",
    "        \n",
    "        \n",
    "    def cond_mut_info(self,target,test,cond,data_in):\n",
    "        # calculate the conditional mutual information between target and test given data_in - I(target:test|cond)\n",
    "        # just uses input data, but have to bin data (creating discrete distribution) to perform calculations\n",
    "        \n",
    "        n_data = len(data_in)\n",
    "        \n",
    "        data_in_np = np.asarray([[item2.item() for item2 in item] for item in data_in])        \n",
    "        cond_temp = cond\n",
    "        \n",
    "        if not cond:\n",
    "            # find parents of target\n",
    "            for item in target:\n",
    "                for item2 in self.parent_name_dict[item]:\n",
    "                    if item2 not in cond_temp:\n",
    "                        cond_temp.append(item2)\n",
    "        \n",
    "        \n",
    "        target_inds = [self.entity_list.index(item) for item in target]\n",
    "        test_inds = [self.entity_list.index(item) for item in test]\n",
    "        cond_inds = [self.entity_list.index(item) for item in cond_temp]\n",
    "\n",
    "        total_inds = target_inds + test_inds + cond_inds\n",
    "        n_tot = len(total_inds)\n",
    "        n_target = len(target_inds)\n",
    "        n_test = len(test_inds)\n",
    "        n_cond = len(cond_inds)\n",
    "\n",
    "        # bin the incoming data\n",
    "        data_bin = np.histogramdd((data_in_np[:,total_inds]),bins=10)[0]/n_data\n",
    "\n",
    "        \n",
    "        # calculate each joint entropy\n",
    "        \n",
    "        all_inds = list(range(0,n_tot))\n",
    "        \n",
    "        p_z = np.sum(data_bin,tuple(all_inds[:n_target+n_test]))\n",
    "        H_z = np.sum(p_z*np.log(p_z+1e-6))\n",
    "        \n",
    "        p_xz = np.sum(data_bin,tuple(all_inds[n_target:n_target+n_test]))\n",
    "        H_xz = np.sum(p_xz*np.log(p_xz+1e-6))\n",
    "        \n",
    "        p_yz = np.sum(data_bin,tuple(all_inds[:n_target]))\n",
    "        H_yz = np.sum(p_yz*np.log(p_yz+1e-6))\n",
    "        \n",
    "        H_xyz = np.sum(data_bin*np.log(data_bin+1e-6))\n",
    "                \n",
    "        return H_xz + H_yz - H_xyz - H_z\n",
    "        \n",
    "    def g_test(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest \n",
    "        # determine if causal graph captures underlying distribution\n",
    "        # have to bin data to perform calculations\n",
    "        \n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        \n",
    "        if self.node_dict[name[0]].node_type == 'binary':\n",
    "            # bin the data\n",
    "            binned_data = torch.histc(data_in[:,name_ind],2,-0.5,1.5)\n",
    "            \n",
    "            # generate sample data\n",
    "            data_samp = torch.tensor([self.model_sample()[name[0]] for i in range(0,len(data_in))])\n",
    "            binned_samp = torch.histc(data_samp,2,-0.5,1.5)\n",
    "            \n",
    "        else:\n",
    "            data_max = torch.max(data_in[:,name_ind])\n",
    "            data_min = torch.min(data_in[:,name_ind])\n",
    "\n",
    "            # bin the data\n",
    "            binned_data = torch.histc(data_in[:,name_ind],100,data_min,data_max)\n",
    "\n",
    "            # generate sample data\n",
    "            data_samp = torch.tensor([self.model_sample()[name[0]] for i in range(0,len(data_in))])\n",
    "            binned_samp = torch.histc(data_samp,100,data_min,data_max)\n",
    "        \n",
    "        \n",
    "        g_val = 2*torch.sum(binned_data*torch.log(binned_data/(binned_samp+1e-6)))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "        \n",
    "  \n",
    "    def write_to_cf(self,filename,spacing):\n",
    "        # write the causal graph to a text file to import into causal fusion\n",
    "        \n",
    "        pos_dict = nx.drawing.layout.planar_layout(self.graph)\n",
    "        \n",
    "        write_dict = {}\n",
    "        write_dict['name'] = 'causal_graph'\n",
    "        \n",
    "        # write nodes\n",
    "        write_dict['nodes'] = []\n",
    "        for i in range(0,len(self.entity_list)):\n",
    "            name = self.entity_list[i]\n",
    "            \n",
    "            write_dict['nodes'].append({})\n",
    "            \n",
    "            write_dict['nodes'][-1]['id'] = 'node' + str(i)\n",
    "            write_dict['nodes'][-1]['name'] = name\n",
    "            write_dict['nodes'][-1]['label'] = name\n",
    "            write_dict['nodes'][-1]['type'] = 'basic'\n",
    "            write_dict['nodes'][-1]['metadata'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['x'] = spacing*pos_dict[i][0]\n",
    "            write_dict['nodes'][-1]['metadata']['y'] = spacing*pos_dict[i][1]\n",
    "            write_dict['nodes'][-1]['metadata']['label'] = ''\n",
    "            write_dict['nodes'][-1]['metadata']['shape'] = 'ellipse'\n",
    "            write_dict['nodes'][-1]['metadata']['fontSize'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['sizeLabelMode'] = 5\n",
    "            write_dict['nodes'][-1]['metadata']['font'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['font']['size'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['size'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeId'] = 'node' + str(i) + 'ID'\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset']['y'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset']['y'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['color'] = '#00000080'\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['size'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['y'] = 0\n",
    "            \n",
    "        # write edges\n",
    "        write_dict['edges'] = []\n",
    "        \n",
    "        for i in range(0,len(self.edge_list)):\n",
    "            \n",
    "            item = self.edge_list[i]\n",
    "            from_node = self.entity_list.index(item[0])\n",
    "            to_node = self.entity_list.index(item[1])\n",
    "            \n",
    "            write_dict['edges'].append({})\n",
    "            \n",
    "            write_dict['edges'][-1]['id'] = 'node' + str(from_node) + '->node' + str(to_node)\n",
    "            write_dict['edges'][-1]['from'] = item[0]\n",
    "            write_dict['edges'][-1]['to'] = item[1]\n",
    "            write_dict['edges'][-1]['type'] = 'directed'\n",
    "            write_dict['edges'][-1]['metadata'] = {}\n",
    "            write_dict['edges'][-1]['metadata']['isLabelDraggable'] = True\n",
    "            write_dict['edges'][-1]['metadata']['label'] = ''\n",
    "            \n",
    "        \n",
    "        write_dict['task'] = {}\n",
    "        \n",
    "        write_dict['metadata'] = {}\n",
    "        \n",
    "        write_dict['project_id'] = '123456789'\n",
    "        write_dict['_fileType'] = 'graph'\n",
    "                \n",
    "        with open(filename + '.json', 'w') as json_file:\n",
    "            json.dump(write_dict, json_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causal graph is a acyclic\n"
     ]
    }
   ],
   "source": [
    "str_list = ['temp =| cloudy','cloudy => rainy','temp => icream','rainy =| icream']\n",
    "type_dict = {}\n",
    "type_dict['temp'] = 'continuous'\n",
    "type_dict['cloudy'] = 'continuous'\n",
    "type_dict['rainy'] = 'continuous'\n",
    "type_dict['icream'] = 'continuous'\n",
    "graph_test = cg_graph(str_list=str_list,type_dict=type_dict,only_creases=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bel_temp = pb.from_bel_script('sag_bel_graph.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causal graph is a acyclic\n"
     ]
    }
   ],
   "source": [
    "graph_test = cg_graph(bel_graph=bel_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvYhIjlOhFxHJcSr0IiI5ToVeRCTH/X9EjclGfGIZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7520)\n",
      "\n",
      "cloudy\n",
      "tensor(-0.2561, grad_fn=<NegBackward>)\n",
      "\n",
      "rainy\n",
      "tensor(1.0858, grad_fn=<NegBackward>)\n",
      "\n",
      "icream\n",
      "tensor(-1.4748, grad_fn=<NegBackward>)\n",
      "\n",
      "['temp']\n"
     ]
    }
   ],
   "source": [
    "graph_test.prob_init(tot_data)\n",
    "print(graph_test.exog_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex(a(CHEBI:26667 ! sialic acid), p(HGNC:1659 ! CD33))\n",
      "continuous\n",
      "\n",
      "p(HGNC:1659 ! CD33)\n",
      "continuous\n",
      "\n",
      "a(CHEBI:26667 ! sialic acid)\n",
      "continuous\n",
      "\n",
      "p(HGNC:1659 ! CD33, pmod(Ph))\n",
      "continuous\n",
      "\n",
      "p(HGNC:9658 ! PTPN6)\n",
      "continuous\n",
      "\n",
      "p(HGNC:9644 ! PTPN11)\n",
      "continuous\n",
      "\n",
      "p(HGNC:11491 ! SYK)\n",
      "continuous\n",
      "\n",
      "p(HGNC:17761 ! TREM2)\n",
      "continuous\n",
      "\n",
      "p(HGNC:12449 ! TYROBP)\n",
      "continuous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(graph_test.node_dict[item].name)\n",
    "    print(graph_test.node_dict[item].node_type)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(graph_test.node_dict[item].alpha_j)\n",
    "    print(graph_test.node_dict[item].alpha_jk)\n",
    "    print(graph_test.node_dict[item].beta_j)\n",
    "    print(graph_test.node_dict[item].beta_jk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_test.write_to_cf('sag_graph',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in graph_test.node_dict:\n",
    "    print(item)\n",
    "    print(graph_test.node_dict[item].node_type)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cloudy': tensor(0.6033, grad_fn=<SqueezeBackward0>),\n",
       " 'icream': tensor(0.0011, grad_fn=<SqueezeBackward0>),\n",
       " 'rainy': tensor(1.5987, grad_fn=<SqueezeBackward0>),\n",
       " 'temp': tensor(15.0858)}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_test.model_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n",
      "tensor(18.7738)\n",
      "\n",
      "cloudy\n",
      "tensor(0.5000)\n",
      "\n",
      "rainy\n",
      "tensor(1.5028, grad_fn=<SqueezeBackward0>)\n",
      "\n",
      "icream\n",
      "tensor(2.1478, grad_fn=<SqueezeBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cond_dict = {}\n",
    "cond_dict['cloudy'] = torch.Tensor([0.5])\n",
    "\n",
    "cond_test = graph_test.model_cond_sample(cond_dict)\n",
    "\n",
    "for item in cond_test:\n",
    "    print(item)\n",
    "    print(cond_test[item])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n",
      "tensor(15.0768)\n",
      "\n",
      "cloudy\n",
      "tensor(0.6302, grad_fn=<SqueezeBackward0>)\n",
      "\n",
      "rainy\n",
      "tensor(2.5000)\n",
      "\n",
      "icream\n",
      "tensor(0.1782, grad_fn=<SqueezeBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_dict = {}\n",
    "do_dict['rainy'] = torch.Tensor([2.5])\n",
    "\n",
    "do_test = graph_test.model_do_sample(do_dict)\n",
    "\n",
    "for item in do_test:\n",
    "    print(item)\n",
    "    print(do_test[item])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n",
      "tensor(18.5983)\n",
      "\n",
      "cloudy\n",
      "tensor(0.5000)\n",
      "\n",
      "rainy\n",
      "tensor(2.5000)\n",
      "\n",
      "icream\n",
      "tensor(0.1034, grad_fn=<SqueezeBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_cond_test = graph_test.model_do_cond_sample(do_dict,cond_dict)\n",
    "for item in do_cond_test:\n",
    "    print(item)\n",
    "    print(do_cond_test[item])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n",
      "tensor(17.4324)\n",
      "\n",
      "cloudy\n",
      "tensor(0.0435, grad_fn=<SqueezeBackward0>)\n",
      "\n",
      "rainy\n",
      "tensor(1.5000)\n",
      "\n",
      "icream\n",
      "tensor(0.0931, grad_fn=<SqueezeBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs_dict = {}\n",
    "obs_dict['icream'] = torch.Tensor([0.5])\n",
    "obs_dict['rainy'] = torch.Tensor([0.8])\n",
    "\n",
    "do_dict = {}\n",
    "do_dict['rainy'] = torch.Tensor([1.5])\n",
    "\n",
    "\n",
    "counter_test = graph_test.model_counterfact(obs_dict,do_dict)\n",
    "for item in counter_test:\n",
    "    print(item)\n",
    "    print(counter_test[item])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15610234126017097\n"
     ]
    }
   ],
   "source": [
    "print(graph_test.cond_mut_info(['icream'],['rainy'],['temp'],tot_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(6026.3399), 1.0)\n"
     ]
    }
   ],
   "source": [
    "a = graph_test.g_test(['icream'],tot_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(graph_test))\n",
    "print()\n",
    "print(graph_test.entity_list)\n",
    "print(graph_test.adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indep_vars(n_samples):\n",
    "    \n",
    "    T_list = []\n",
    "    C_list = []\n",
    "    P_list = []\n",
    "    \n",
    "    for i in range(0,n_samples):\n",
    "        \n",
    "        #x = pyro.sample(\"x_{}\".format(i), pyro.distributions.Normal(20,5))\n",
    "        \n",
    "        #T_temp = pyro.distributions.Normal(20,5).sample()\n",
    "        #C_temp = 0.5*pyro.distributions.Beta(1,1+T_temp/10).sample() + 0.5*pyro.distributions.Uniform(0,1).sample()\n",
    "        #P_temp = (0.5*pyro.distributions.Exponential(1).sample() \n",
    "            #+ 0.5*pyro.distributions.Exponential(1/(C_temp+1)).sample())\n",
    "        \n",
    "        T_list.append(pyro.sample(\"T_{}\".format(i), pyro.distributions.LogNormal(2.96,0.2)))\n",
    "        \n",
    "        C_list.append(0.5*pyro.sample(\"C1_{}\".format(i),pyro.distributions.Beta(1,1+T_list[-1]/10)) \n",
    "            + 0.5*pyro.sample(\"C2_{}\".format(i),pyro.distributions.Uniform(0,1)))\n",
    "        P_list.append(0.5*pyro.sample(\"P1_{}\".format(i), pyro.distributions.Exponential(1))\n",
    "            + 0.5*pyro.sample(\"P2.{}\".format(i),pyro.distributions.Exponential(1/(C_list[-1]+1))))\n",
    "        \n",
    "    return T_list,C_list,P_list\n",
    "\n",
    "def dep_vars(T_list,C_list,P_list):\n",
    "    \n",
    "    n_pts = len(T_list)\n",
    "    \n",
    "    I_list = []\n",
    "    \n",
    "    for i in range(0,n_pts):\n",
    "        \n",
    "        T_temp = T_list[i]\n",
    "        C_temp = C_list[i]\n",
    "        P_temp = P_list[i]\n",
    "        \n",
    "        if P_temp > 2.5 or T_temp < 15:\n",
    "            I_list.append(pyro.sample(\"I_{}\".format(i),pyro.distributions.Bernoulli(0))+1e-6)\n",
    "        else:\n",
    "            I_list.append(pyro.sample(\"I_{}\".format(i),\n",
    "                pyro.distributions.Beta(2*(2.5-P_temp)*(T_temp-12)/(2.5*12),2))+1e-6)\n",
    "            #I_temp = torch.tensor(0.5)\n",
    "        \n",
    "    return I_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "temp,cloud,precip = indep_vars(n_data)\n",
    "icream = dep_vars(temp,cloud,precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 4])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "tot_data = torch.Tensor([temp,cloud,precip,icream])\n",
    "tot_data = tot_data.T\n",
    "print(tot_data.size())\n",
    "print(tot_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.sum(torch.lt(tot_data,1e-6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46128135e+01 3.81122879e-01 9.47273093e-01 1.00000000e-06]\n",
      " [2.39220507e+01 5.79113534e-01 1.73934534e+00 2.86442721e-01]\n",
      " [2.02819826e+01 2.81136566e-01 1.75762778e+00 6.62767430e-01]\n",
      " ...\n",
      " [2.34347768e+01 1.67162027e-01 1.35281624e+00 6.14039732e-01]\n",
      " [1.55175092e+01 4.51119644e-01 1.17293660e+00 1.13925318e-04]\n",
      " [1.93683867e+01 1.10350912e-01 3.65258173e-01 3.23652825e-01]]\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "tot_data_np = np.asarray([[item2.item() for item2 in item] for item in tot_data])\n",
    "print(tot_data_np)\n",
    "print(type(tot_data_np))\n",
    "print(np.shape(tot_data_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
